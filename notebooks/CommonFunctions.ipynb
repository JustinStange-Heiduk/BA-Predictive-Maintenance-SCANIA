{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c9814d",
   "metadata": {},
   "source": [
    "# Predictive Maintenance mit SCANIA-Daten – Common Functions to load\n",
    "\n",
    "**Projekt:** Bachelorarbeit Data Science  \n",
    "**Thema:** \n",
    "**Datengrundlage:** SCANIA Component X Dataset  \n",
    "**Autor:** Justin Stange-Heiduk  \n",
    "**Betreuung:** Dr. Martin Prause  \n",
    "**Ziel:** Erstellen und testen der Daten Vorbereitung Funktionen  \n",
    "\n",
    "---\n",
    "\n",
    "**Erstellt:** 2025-08-19   \n",
    "**Letzte Änderung:** 2025-07-25\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01563f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b7387be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df: pd.DataFrame, ordner: str, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Speichert ein DataFrame als Parquet-Datei im angegebenen Ordner.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Das zu speichernde DataFrame.\n",
    "        ordner (str): Der Ordner, in dem die Parquet-Datei gespeichert werden soll.\n",
    "        name (str): Der Name der Parquet-Datei (ohne .parquet)\n",
    "    \"\"\"\n",
    "\n",
    "    df.to_parquet(f\"../data/{ordner}/{name}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20db674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(ordner: str, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lädt ein DataFrame aus einer Parquet-Datei im angegebenen Ordner.\n",
    "\n",
    "    Args:\n",
    "        ordner (str): Der Ordner, in dem die Parquet-Datei gespeichert ist. (../data/{ordner})\n",
    "        name (str): Der Name der Parquet-Datei (ohne .parquet)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Das geladene DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.read_parquet(f\"../data/{ordner}/{name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5334e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_dd(ordner: str, name: str) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lädt ein DataFrame aus einer Parquet-Datei im angegebenen Ordner.\n",
    "\n",
    "    Args:\n",
    "        ordner (str): Der Ordner, in dem die Parquet-Datei gespeichert ist. (../data/{ordner})\n",
    "        name (str): Der Name der Parquet-Datei (ohne .parquet)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Das geladene DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    return dd.read_parquet(f\"../data/{ordner}/{name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd868f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_raw_data() -> dict:\n",
    "    \"\"\"\n",
    "    Load raw data from a CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The loaded raw data.\n",
    "    \"\"\"\n",
    "    test_labels = pd.read_csv(\"../data/01_raw/test_labels.csv\")\n",
    "    test_operational = pd.read_csv(\"../data/01_raw/test_operational_readouts.csv\")\n",
    "    test_specifications = pd.read_csv(\"../data/01_raw/test_specifications.csv\")\n",
    "\n",
    "    train_tte = pd.read_csv(\"../data/01_raw/train_tte.csv\")\n",
    "    train_operational = pd.read_csv(\"../data/01_raw/train_operational_readouts.csv\")\n",
    "    train_specifications = pd.read_csv(\"../data/01_raw/train_specifications.csv\")\n",
    "\n",
    "    validation_tte = pd.read_csv(\"../data/01_raw/validation_labels.csv\")\n",
    "    validation_operational = pd.read_csv(\"../data/01_raw/validation_operational_readouts.csv\")\n",
    "    validation_specifications = pd.read_csv(\"../data/01_raw/validation_specifications.csv\")\n",
    "\n",
    "    return dict({\"test\": {\"labels\": test_labels, \"readouts\": test_operational, \"spec\": test_specifications},\n",
    "           \"train\": {\"tte\": train_tte, \"readouts\": train_operational, \"spec\": train_specifications},\n",
    "           \"validation\": {\"labels\": validation_tte, \"readouts\": validation_operational, \"spec\": validation_specifications}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8d1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_specific_raw_data(name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load specific raw data from CSV files based on the data type.\n",
    "\n",
    "    Args:\n",
    "        name (str): The type of data to load. Options are:\n",
    "        test_labels, test_operational_readouts, test_specifications, \n",
    "        train_tte, train_operational_readouts, train_specifications, \n",
    "        validation_labels, validation_operational_readouts, validation_specifications.\n",
    "\n",
    "    Returns:\n",
    "        dict: The loaded raw data for the specified data type.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(f\"../data/01_raw/{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977719f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_rsf_model_input\u001b[39m(df: \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame, columns_to_drop: \u001b[38;5;28mlist\u001b[39m, frag: \u001b[38;5;28mfloat\u001b[39m, class_column: \u001b[38;5;28mstr\u001b[39m, sampling: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[pd\u001b[38;5;241m.\u001b[39mDataFrame, np\u001b[38;5;241m.\u001b[39mndarray]: \n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Prepares the input data for the Random Survival Forest model with option to sample a fraction of each class. \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Args: df (pd.DataFrame): The input dataframe containing features and target variables. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    sampling (bool): Whether to perform sampling or not.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    Returns: tuple[pd.DataFrame, np.ndarray]: A tuple containing the feature dataframe and the structured array for survival analysis. \"\"\"\u001b[39;00m \n\u001b[1;32m     10\u001b[0m     df_list \u001b[38;5;241m=\u001b[39m [] \n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def prepare_rsf_model_input(df: pd.DataFrame, columns_to_drop: list, frag: float, class_column: str, sampling: bool) -> tuple[pd.DataFrame, np.ndarray]: \n",
    "    \"\"\" Prepares the input data for the Random Survival Forest model with option to sample a fraction of each class. \n",
    "    \n",
    "    Args: df (pd.DataFrame): The input dataframe containing features and target variables. \n",
    "    columns_to_drop (list): List of columns to drop from the dataframe. \n",
    "    frag (float): Fraction of data to sample from each class. \n",
    "    class_column (str): The name of the column representing the class labels. \n",
    "    sampling (bool): Whether to perform sampling or not.\n",
    "    Returns: tuple[pd.DataFrame, np.ndarray]: A tuple containing the feature dataframe and the structured array for survival analysis. \"\"\" \n",
    "    df_list = [] \n",
    "\n",
    "    if sampling:\n",
    "        for i in df[class_column].unique(): \n",
    "            df_list.append( df[df[class_column] == i].sample(frac=frag, random_state=42)) \n",
    "        df = pd.concat(df_list) \n",
    "\n",
    "    y_surv = Surv.from_arrays(event=df[\"event\"].astype(bool), time=df[\"duration\"].astype(float)) \n",
    "    X = df.drop(columns=columns_to_drop) \n",
    "    return X, y_surv \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rsf_model_input(df: pd.DataFrame, columns_to_drop) -> pd.DataFrame: \n",
    "    \"\"\" Prepares the input data for the XGBoost model with aft. \n",
    "    \n",
    "    Args: \n",
    "    df (pd.DataFrame): The input dataframe containing features and target variables. \n",
    "    columns_to_drop (list): List of columns to drop from the dataframe. \n",
    "\n",
    "    Return:\n",
    "    pd.DataFrame: The feature dataframe for XGBoost. \n",
    "    \"\"\"\n",
    "\n",
    "    y = {\n",
    "        \"lower_bound\": df[\"duration\"].astype(float),\n",
    "        \"upper_bound\":  df[\"upper_bound\"].astype(float),\n",
    "    }\n",
    "\n",
    "    x = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    d = xgb.DMatrix(data=x, label_lower_bound=y[\"lower_bound\"],\n",
    "                     label_upper_bound=y[\"upper_bound\"])\n",
    "\n",
    "    return d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
