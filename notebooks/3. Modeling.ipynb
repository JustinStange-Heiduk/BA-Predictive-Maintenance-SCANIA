{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dc4e129",
   "metadata": {},
   "source": [
    "# Predictive Maintenance mit SCANIA-Daten – Modeling\n",
    "\n",
    "**Projekt:** Bachelorarbeit Data Science  \n",
    "**Thema:** \n",
    "**Datengrundlage:** SCANIA Component X Dataset  \n",
    "**Autor:** Justin Stange-Heiduk  \n",
    "**Betreuung:** Dr. Martin Prause  \n",
    "**Ziel:** Modell erstellung XGBoost mit AFT und Random Forest Survival  \n",
    "\n",
    "---\n",
    "\n",
    "**Erstellt:** 2025-09-01  \n",
    "**Letzte Änderung:** 2025-09-25\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5eff8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_censored, integrated_brier_score\n",
    "import mlflow\n",
    "import optuna\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time, sys\n",
    "from optuna.samplers import GridSampler\n",
    "from hashlib import sha1\n",
    "import xgboost as xgb\n",
    "import scipy\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9114bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"0. CommonFunctions.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aad7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e9e24",
   "metadata": {},
   "source": [
    "### 1. Random Survival Forest HPO\n",
    "### 2. XGBoost mit AFT HPO\n",
    "### 3. Modellerstellung RSF\n",
    "### 4. Modelerstellung XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad06a607",
   "metadata": {},
   "source": [
    "### 1. Random Survival Forest HPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dab01db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "COST, TAUS = get_cost_and_taus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9d64a5",
   "metadata": {},
   "source": [
    "#### Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0ef421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rsf_model_input(df: pd.DataFrame, columns_to_drop: list, frag: float, class_column: str, sampling: bool) -> tuple[pd.DataFrame, np.ndarray]: \n",
    "    \"\"\" Prepares the input data for the Random Survival Forest model with option to sample a fraction of each class. \n",
    "    \n",
    "    Args: df (pd.DataFrame): The input dataframe containing features and target variables. \n",
    "    columns_to_drop (list): List of columns to drop from the dataframe. \n",
    "    frag (float): Fraction of data to sample from each class. \n",
    "    class_column (str): The name of the column representing the class labels. \n",
    "    sampling (bool): Whether to perform sampling or not.\n",
    "    Returns: tuple[pd.DataFrame, np.ndarray]: A tuple containing the feature dataframe and the structured array for survival analysis. \"\"\" \n",
    "    df_list = [] \n",
    "\n",
    "    if sampling:\n",
    "        for i in df[class_column].unique(): \n",
    "            df_list.append( df[df[class_column] == i].sample(frac=frag, random_state=42)) \n",
    "        df = pd.concat(df_list) \n",
    "\n",
    "    y_surv = Surv.from_arrays(event=df[\"event\"].astype(bool), time=df[\"duration\"].astype(float)) \n",
    "    X = df.drop(columns=columns_to_drop) \n",
    "    return X, y_surv \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "673bd616",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train_surv = prepare_rsf_model_input(load_df(ordner=\"04_feature\", name = \"feature_train_corr_labels\").drop(columns=[\"upper_bound\"]), columns_to_drop=[\"duration\", \"event\", \"vehicle_id\", \"class\"], frag=0.01, class_column=\"class\", sampling=True) \n",
    "\n",
    "X_val, y_val_surv = prepare_rsf_model_input(load_df(ordner=\"04_feature\", name = \"feature_validation_corr_labels\").drop(columns=[\"upper_bound\"]), columns_to_drop=[\"duration\", \"event\", \"vehicle_id\", \"class_label\"], frag=1.0, class_column=\"class_label\", sampling=False) \n",
    "\n",
    "validation_data = load_df(ordner=\"04_feature\", name = \"feature_validation_corr_labels\").drop(columns=[\"upper_bound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d6210c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Sicherstellen, dass der Ordner existiert\n",
    "save_dir = \"../Data/05_model_input/HPO_RSF\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# X_train und X_val (DataFrames) speichern\n",
    "X_train.to_parquet(os.path.join(save_dir, \"X_train.parquet\"), index=False)\n",
    "X_val.to_parquet(os.path.join(save_dir, \"X_val.parquet\"), index=False)\n",
    "\n",
    "# y_train_surv und y_val_surv sind Structured Arrays -> DataFrame\n",
    "y_train_df = pd.DataFrame({\n",
    "    \"event\": y_train_surv[\"event\"].astype(int),\n",
    "    \"duration\": y_train_surv[\"time\"].astype(float)\n",
    "})\n",
    "y_val_df = pd.DataFrame({\n",
    "    \"event\": y_val_surv[\"event\"].astype(int),\n",
    "    \"duration\": y_val_surv[\"time\"].astype(float)\n",
    "})\n",
    "\n",
    "y_train_df.to_parquet(os.path.join(save_dir, \"y_train_surv.parquet\"), index=False)\n",
    "y_val_df.to_parquet(os.path.join(save_dir, \"y_val_surv.parquet\"), index=False)\n",
    "\n",
    "# Validation Data auch speichern\n",
    "validation_data.to_parquet(os.path.join(save_dir, \"validation_data.parquet\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_parquet(\"../Data/05_model_input/HPO_RSF/X_train.parquet\")\n",
    "X_val   = pd.read_parquet(\"../Data/05_model_input/HPO_RSF/X_val.parquet\")\n",
    "y_train_df = pd.read_parquet(\"../Data/05_model_input/HPO_RSF/y_train_surv.parquet\")\n",
    "y_val_df   = pd.read_parquet(\"../Data/05_model_input/HPO_RSF/y_val_surv.parquet\")\n",
    "\n",
    "# Zurück in Surv-Format\n",
    "y_train_surv = Surv.from_arrays(event=y_train_df[\"event\"].astype(bool),\n",
    "                                time=y_train_df[\"duration\"].astype(float))\n",
    "y_val_surv   = Surv.from_arrays(event=y_val_df[\"event\"].astype(bool),\n",
    "                                time=y_val_df[\"duration\"].astype(float))\n",
    "\n",
    "validation_data = pd.read_parquet(\"../Data/05_model_input/HPO_RSF/validation_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b6b91",
   "metadata": {},
   "source": [
    "#### Sichtbare Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "030fd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(msg):\n",
    "    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4db4c2",
   "metadata": {},
   "source": [
    "#### Kostenfunktion und Klassen-Mapping aus der Survivalkurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10ef77b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_and_taus()-> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\" Returns the cost matrix and class boundaries (taus) for RUL classification.    \n",
    "    # Kostenmatrix aus deinem Paper (Zeilen = Actual n, Spalten = Predicted m)\n",
    "    Returns: tuple[np.ndarray, np.ndarray]: A tuple containing the cost matrix and class boundaries. \n",
    "    \"\"\"\n",
    "    COST = np.array([\n",
    "        [0,   7,   8,   9,   10],\n",
    "        [200, 0,   7,   8,    9],\n",
    "        [300, 200, 0,   7,    8],\n",
    "        [400, 300, 200, 0,    7],\n",
    "        [500, 400, 300, 200,  0]\n",
    "    ], dtype=float)\n",
    "\n",
    "    # Klassengrenzen für RUL in Zeiteinheiten, konsistent zu deinen Labels 0..4\n",
    "    # Beispiel: 4: [0,6), 3: [6,12), 2: [12,24), 1: [24,48), 0: [48, inf)\n",
    "    TAUS = np.array([6.0, 12.0, 24.0, 48.0], dtype=float)\n",
    "\n",
    "    return COST, TAUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4b8aee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probs_from_S_tau(S_tau: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Berechnet p0..p4 direkt aus den Werten S(tau1..tau4).\n",
    "\n",
    "    Args: \n",
    "        S_tau = [S(tau1), S(tau2), S(tau3), S(tau4)].\n",
    "        \n",
    "    Return: Wahrscheinlichkeiten p0..p4 für Klassen 0..4\n",
    "    \"\"\"\n",
    "    S1, S2, S3, S4 = S_tau\n",
    "    p4 = 1.0 - S1\n",
    "    p3 = S1 - S2\n",
    "    p2 = S2 - S3\n",
    "    p1 = S3 - S4\n",
    "    p0 = S4\n",
    "    p = np.clip(np.array([p0, p1, p2, p3, p4], dtype=float), 0.0, 1.0)\n",
    "    s = p.sum()\n",
    "    return p / s if s > 0 else np.array([1.0, 0.0, 0.0, 0.0, 0.0], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "935c5059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_with_cost_from_rsf_at_taus(\n",
    "    rsf: RandomSurvivalForest,\n",
    "    X: pd.DataFrame,\n",
    "    taus: np.ndarray,\n",
    "    cost: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Ermittelt je Instanz die kostenminimale Klasse m̂ und die dazugehörigen Größen.\n",
    "\n",
    "    Args:\n",
    "        rsf: trainiertes RandomSurvivalForest Modell\n",
    "        X: Eingabedaten (Features) der Form (N, n_features)\n",
    "        taus: Klassengrenzen der Form (4,)\n",
    "        cost: Kostenmatrix der Form (5,5)\n",
    "        \n",
    "    Rückgaben:\n",
    "      pred_class  Länge N\n",
    "      exp_cost_min Länge N\n",
    "      probs       Form (N,5) für p0..p4\n",
    "    \"\"\"\n",
    "    surv_fns = rsf.predict_survival_function(X, return_array=False)\n",
    "    N = len(X)\n",
    "    pred_class = np.zeros(N, dtype=int)\n",
    "    exp_cost_min = np.zeros(N, dtype=float)\n",
    "    probs = np.zeros((N, 5), dtype=float)\n",
    "\n",
    "    for i, fn in enumerate(surv_fns):\n",
    "        S_tau = fn(taus)\n",
    "        p = class_probs_from_S_tau(S_tau)\n",
    "        exp_vec = cost.T @ p\n",
    "        m_hat = int(np.argmin(exp_vec))\n",
    "        pred_class[i] = m_hat\n",
    "        exp_cost_min[i] = float(exp_vec[m_hat])\n",
    "        probs[i, :] = p\n",
    "\n",
    "    return pred_class, exp_cost_min, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7d1790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_decision_costs_from_true(\n",
    "    true_class: pd.Series | np.ndarray,\n",
    "    pred_class: np.ndarray,\n",
    "    cost: np.ndarray\n",
    ") -> tuple[float, float, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Berechnet realisierte Kosten Cost[n, m̂] und Konfusion.\n",
    "\n",
    "    Args:\n",
    "        true_class: wahre Klassen der Form (N,)\n",
    "        pred_class: vorhergesagte Klassen der Form (N,)\n",
    "        cost: Kostenmatrix der Form (5,5)\n",
    "        \n",
    "    Rückgaben:\n",
    "      avg_cost, total_cost\n",
    "    \"\"\"\n",
    "    n = np.asarray(true_class, dtype=int)\n",
    "    m = np.asarray(pred_class, dtype=int)\n",
    "    assert n.shape == m.shape, \"true_class und pred_class müssen gleich lang sein.\"\n",
    "\n",
    "    realized = np.array([cost[n_i, m_i] for n_i, m_i in zip(n, m)], dtype=float)\n",
    "    avg_cost = float(np.mean(realized))\n",
    "    total_cost = float(np.sum(realized))\n",
    "\n",
    "\n",
    "\n",
    "    return avg_cost, total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d40c067",
   "metadata": {},
   "source": [
    "#### RSF-Modell und Metriken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52cfa10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rsf(X: pd.DataFrame, duration: pd.Series, event: pd.Series, **params) -> RandomSurvivalForest:\n",
    "    \"\"\"\n",
    "    Trainiere einen RandomSurvivalForest mit den gegebenen Hyperparametern.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Die Eingabedaten.\n",
    "        duration (pd.Series): Die Überlebenszeiten.\n",
    "        event (pd.Series): Die Ereignisdaten.\n",
    "        **params: Zusätzliche Hyperparameter für das Modell.\n",
    "\n",
    "    Return:\n",
    "        RandomSurvivalForest: Das trainierte Modell.\n",
    "    \"\"\"\n",
    "    \n",
    "    y = Surv.from_arrays(event=event.astype(bool), time=duration.astype(float))\n",
    "    rsf = RandomSurvivalForest(\n",
    "        n_estimators=params.get(\"n_estimators\"),\n",
    "        max_depth=params.get(\"max_depth\"),\n",
    "        max_features=params.get(\"max_features\"),\n",
    "        min_samples_split=params.get(\"min_samples_split\"),\n",
    "        min_samples_leaf=params.get(\"min_samples_leaf\"),\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    log(f\"→ starte FIT mit Parametern: {params}\")\n",
    "    rsf.fit(X, y)\n",
    "    log(\"✓ FIT fertig\")\n",
    "    return rsf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea868843",
   "metadata": {},
   "source": [
    "#### MLflow initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d175bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_mlflow(experiment_name: str, tracking_uri: str | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Setzt MLflow konsistent auf.\n",
    "    - tracking_uri kann sein:\n",
    "      * None           -> nutzt ./mlruns (wird angelegt)\n",
    "      * lokaler Pfad   -> z.B. '/workspace/mlruns' (wird angelegt)\n",
    "      * file-URI       -> z.B. 'file:///workspace/mlruns' (wird angelegt)\n",
    "      * Remote/DB      -> z.B. 'http://...', 'https://...', 'sqlite:///mlflow.db' (kein Ordner nötig)\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): Name des MLflow-Experiments.\n",
    "        tracking_uri (str | None): URI für das Tracking. Siehe Beschreibung.\n",
    "    \n",
    "    \"\"\"\n",
    "    if tracking_uri is None:\n",
    "        root = Path(\"../mlruns\")\n",
    "        root.mkdir(parents=True, exist_ok=True)\n",
    "        mlflow.set_tracking_uri(root.resolve().as_uri())\n",
    "    else:\n",
    "        parsed = urlparse(tracking_uri)\n",
    "        if parsed.scheme in (\"\", \"file\"):\n",
    "            # Rohpfad oder file-URI -> lokalen Ordner anlegen\n",
    "            root = Path(parsed.path if parsed.scheme == \"file\" else tracking_uri)\n",
    "            root.mkdir(parents=True, exist_ok=True)\n",
    "            mlflow.set_tracking_uri(root.resolve().as_uri())\n",
    "        else:\n",
    "            # http(s), sqlite, postgresql, ...\n",
    "            mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(\"MLflow tracking URI:\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a2cb39",
   "metadata": {},
   "source": [
    "#### Hyperparameter-Suche mit Optuna und MLflow-Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "753c6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_callback_totalcost(study: optuna.Study, trial: optuna.trial.FrozenTrial):\n",
    "    \"\"\"\n",
    "    Konsolenfeedback pro Trial und Logging der bisherigen Best-Gesamtkosten.\n",
    "\n",
    "    Args:\n",
    "    study (optuna.Study): Die aktuelle Studie.\n",
    "    trial (optuna.trial.FrozenTrial): Der aktuelle Trial.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"[Trial {trial.number:03d}] state={trial.state.name} value={trial.value:.4f} best={study.best_value:.4f}\")\n",
    "        print(\"#\"*20)\n",
    "        mlflow.log_metric(\"best_total_cost_so_far\", study.best_value, step=trial.number)\n",
    "\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c985522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Helper: baut params direkt aus GRID\n",
    "def build_params_from_grid(trial, grid: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Liest alle Keys aus 'grid' und erzeugt passende trial.suggest_categorical()-Aufrufe.\n",
    "    Einzelelement-Listen werden als Konstante gesetzt (keine Suggestion).\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): Der aktuelle Trial.\n",
    "        grid (dict): Das Hyperparameter-Gitter.\n",
    "    \n",
    "    Return:\n",
    "        dict: Die erzeugten Hyperparameter.\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    for name, choices in grid.items():\n",
    "        if isinstance(choices, (list, tuple)) and len(choices) > 1:\n",
    "            params[name] = trial.suggest_categorical(name, list(choices))\n",
    "        elif isinstance(choices, (list, tuple)) and len(choices) == 1:\n",
    "            params[name] = choices[0]\n",
    "        else:\n",
    "            # Falls jemand mal einen konstanten Wert statt Liste einträgt\n",
    "            params[name] = choices\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9954493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_params(params: dict) -> str:\n",
    "    return str(sorted(params.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5585ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsf_objective_prunable_total_cost(\n",
    "    trial: optuna.Trial,\n",
    "    X_tr: pd.DataFrame, y_tr_surv,            # Train für Fit\n",
    "    X_val: pd.DataFrame, y_val_class: pd.Series,  # Val für Entscheidung und echte Klasse\n",
    "    grid: dict,\n",
    "    TRIED_HASHES: set, \n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Mehrstufiges Objective mit Successive Halving.\n",
    "    Ressource = n_estimators. Auswahlkriterium = realisierte Gesamtkosten auf Val.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.Trial): Der aktuelle Trial.\n",
    "        X_tr (pd.DataFrame): Trainingsdaten für das Fit.\n",
    "        y_tr_surv: Überlebensdaten für das Fit.\n",
    "        X_val (pd.DataFrame): Validierungsdaten für die Entscheidung.\n",
    "        y_val_class (pd.Series): Wahre Klassen für die Kostenberechnung.\n",
    "        grid (dict): Hyperparameter-Gitter.\n",
    "        TRIED_HASHES (set): Menge der bereits getesteten Param-Kombinationen (Hash).\n",
    "    \n",
    "    Return:\n",
    "        float: Die besten realisierten Gesamtkosten auf Val.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(X_tr)\n",
    "\n",
    "    min_leaf_fracs = [0.005, 0.01, 0.02]\n",
    "    GRID = {\n",
    "    \"max_depth\": [int(np.log2(N)), int(np.log2(N)) + 4],\n",
    "    \"max_features\": [\"sqrt\", 0.4],\n",
    "    \"min_samples_leaf\": [int(N * f) for f in min_leaf_fracs],\n",
    "    \"n_estimators\":      [8, 16, 32, 64]\n",
    "    }\n",
    "    GRID[\"min_samples_split\"] = [2 * v for v in GRID[\"min_samples_leaf\"]]\n",
    "\n",
    "    # Suchraum der Hyperparameter (ohne n_estimators, das ist unsere Stufen-Ressource)\n",
    "    params = {\n",
    "        \"max_depth\":        trial.suggest_categorical(\"max_depth\", [12,16]),\n",
    "        \"max_features\":     trial.suggest_categorical(\"max_features\", [\"sqrt\", 0.4]),\n",
    "        \"min_samples_leaf\": trial.suggest_categorical(\"min_samples_leaf\",  [int(N * f) for f in min_leaf_fracs]),\n",
    "    }\n",
    "    params[\"min_samples_split\"] = trial.suggest_categorical(\"min_samples_split\", [2 * v for v in GRID[\"min_samples_leaf\"]])\n",
    "    \n",
    "\n",
    "\n",
    "    # Hash berechnen\n",
    "    params_hash = hash_params(params)\n",
    "\n",
    "    # Trial überspringen, wenn schon getestet\n",
    "    if params_hash in TRIED_HASHES:\n",
    "        print(f\"[SKIP] Trial {trial.number} übersprungen – bekannte Param-Kombi: {params}\")\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    TRIED_HASHES.add(params_hash)\n",
    "\n",
    "\n",
    "    rung_trees = tuple(sorted(grid.get(\"n_estimators\")))\n",
    "\n",
    "    best_full_cost = np.inf\n",
    "    best_n_trees   = None\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.set_tag(\"rungs\", str(rung_trees))\n",
    "        \n",
    "\n",
    "        for step, n_trees in enumerate(rung_trees, start=1): \n",
    "            rsf = fit_rsf(\n",
    "                X_tr,\n",
    "                pd.Series(y_tr_surv[\"time\"],  dtype=float),\n",
    "                pd.Series(y_tr_surv[\"event\"], dtype=bool),\n",
    "                n_estimators=n_trees,\n",
    "                **params\n",
    "            )\n",
    "            # Safety: Feature-Ausrichtung\n",
    "            X_eval = X_val.loc[:, rsf.feature_names_in_] if hasattr(rsf, \"feature_names_in_\") else X_val\n",
    "\n",
    "            # FULL-Entscheidung & realisierte Kosten\n",
    "            COST, TAUS = get_cost_and_taus()\n",
    "            pred_full, _, _ = decide_with_cost_from_rsf_at_taus(rsf, X_eval, TAUS, COST)\n",
    "            avg_full, total_full = evaluate_decision_costs_from_true(\n",
    "                true_class=y_val_class, pred_class=pred_full, cost=COST\n",
    "            )\n",
    "\n",
    "            print(f\"[{time.strftime('%H:%M:%S')}] [rung {step}/{len(rung_trees)} FULL] \"\n",
    "                  f\"n_eval={len(X_eval)}  total_cost={total_full:.2f}  avg_cost={avg_full:.4f}\")\n",
    "\n",
    "            # Logging\n",
    "            mlflow.log_metric(\"val_total_cost_full\", total_full, step=step)\n",
    "            mlflow.log_metric(\"val_avg_cost_full\",   avg_full,   step=step)\n",
    "            mlflow.log_metric(\"val_n_eval_full\",     len(X_eval), step=step)\n",
    "            \n",
    "\n",
    "            # Bestes FULL-Ergebnis über die Rungen merken\n",
    "            if total_full < best_full_cost:\n",
    "                best_full_cost = float(total_full)\n",
    "                best_n_trees   = int(n_trees)\n",
    "                best_full_params = dict(params)\n",
    "                best_full_params[\"n_estimators\"] = best_n_trees\n",
    "                trial.set_user_attr(\"best_n_estimators\", best_n_trees)\n",
    "                trial.set_user_attr(\"full_params\", best_full_params)\n",
    "                mlflow.log_metric(\"best_so_far_total_cost_full\", best_full_cost, step=step)\n",
    "                mlflow.set_tag(\"best_so_far_n_estimators\", best_n_trees)\n",
    "\n",
    "            # Pruning basiert ebenfalls auf FULL (du wolltest nur FULL)\n",
    "            if step < len(rung_trees):\n",
    "                trial.report(total_full, step=step)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "    \n",
    "        \n",
    "        return float(best_full_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e39f864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rsf_study_totalcost(\n",
    "    X_train: pd.DataFrame, y_train_surv,\n",
    "    X_val: pd.DataFrame, y_val_class: pd.Series,\n",
    "    experiment_name: str = \"RSF_HPO_TOTALCOST\",\n",
    "    tracking_uri: str | None = None\n",
    ") -> optuna.Study:\n",
    "    \"\"\"\n",
    "    Startet die Optuna-Studie mit Successive Halving und wählt Hyperparameter\n",
    "    strikt nach minimalen REALISIERTEN Gesamtkosten auf dem Validationset.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Trainingsdaten für das Fit.\n",
    "        y_train_surv: Überlebensdaten für das Fit.\n",
    "        X_val (pd.DataFrame): Validierungsdaten für die Entscheidung.\n",
    "        y_val_class (pd.Series): Wahre Klassen für die Kostenberechnung.\n",
    "        experiment_name (str): Name des MLflow-Experiments.\n",
    "        tracking_uri (str | None): URI für das Tracking. Siehe Beschreibung in setup_mlflow().\n",
    "\n",
    "    Return:\n",
    "        optuna.Study: Die durchgeführte Studie.\n",
    "    \"\"\"\n",
    "\n",
    "    TRIED_HASHES = set()\n",
    "\n",
    "    setup_mlflow(experiment_name, tracking_uri)\n",
    "    N = len(X_train)\n",
    "    min_leaf_fracs = [0.005, 0.01, 0.02]\n",
    "    GRID = {\n",
    "    \"max_depth\": [12, 16],\n",
    "    \"max_features\": [\"sqrt\", 0.4],\n",
    "    \"min_samples_leaf\": [int(N * f) for f in min_leaf_fracs],\n",
    "    \"n_estimators\":      [8, 16, 32, 64]\n",
    "    }\n",
    "    GRID[\"min_samples_split\"] = [2 * v for v in GRID[\"min_samples_leaf\"]]\n",
    "    \n",
    "\n",
    "    n_combos = int(np.prod([len(v) for k, v in GRID.items() if k != \"n_estimators\"]))\n",
    "\n",
    "    sampler = GridSampler(search_space=GRID)\n",
    "    pruner = SuccessiveHalvingPruner(min_resource=1, reduction_factor=10)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"rsf_hpo_total_cost_sh\"):\n",
    "        study.optimize(\n",
    "            lambda t: rsf_objective_prunable_total_cost(t, X_train, y_train_surv, X_val, y_val_class, grid=GRID, TRIED_HASHES=TRIED_HASHES),\n",
    "            n_trials=n_combos,\n",
    "            show_progress_bar=True,\n",
    "            callbacks=[progress_callback_totalcost],\n",
    "        )\n",
    "        mlflow.log_params(study.best_trial.user_attrs[\"full_params\"])\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd90fae",
   "metadata": {},
   "source": [
    "#### Anwendung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d1aba14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-09 15:16:47,825] A new study created in memory with name: no-name-1a881dc4-5f60-4810-b1a8-241d155c1198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: file:///workspace/mlruns\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601f08edfb7b4b418cf7b7067362c39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:16:48] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 56, 'min_samples_split': 224}\n",
      "[15:16:52] ✓ FIT fertig\n",
      "[15:16:53] [rung 1/4 FULL] n_eval=5046  total_cost=45998.00  avg_cost=9.1157\n",
      "[15:16:53] → starte FIT mit Parametern: {'n_estimators': 16, 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 56, 'min_samples_split': 224}\n",
      "[15:16:58] ✓ FIT fertig\n",
      "[15:16:59] [rung 2/4 FULL] n_eval=5046  total_cost=48081.00  avg_cost=9.5285\n",
      "[15:17:00] → starte FIT mit Parametern: {'n_estimators': 32, 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 56, 'min_samples_split': 224}\n",
      "[15:17:09] ✓ FIT fertig\n",
      "[15:17:10] [rung 3/4 FULL] n_eval=5046  total_cost=47722.00  avg_cost=9.4574\n",
      "[15:17:11] → starte FIT mit Parametern: {'n_estimators': 64, 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 56, 'min_samples_split': 224}\n",
      "[15:17:29] ✓ FIT fertig\n",
      "[15:17:33] [rung 4/4 FULL] n_eval=5046  total_cost=50142.00  avg_cost=9.9370\n",
      "[I 2025-09-09 15:17:34,128] Trial 0 finished with value: 45998.0 and parameters: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 56, 'min_samples_split': 224}. Best is trial 0 with value: 45998.0.\n",
      "[Trial 000] state=COMPLETE value=45998.0000 best=45998.0000\n",
      "####################\n",
      "[15:17:34] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 224}\n",
      "[15:17:38] ✓ FIT fertig\n",
      "[15:17:38] [rung 1/4 FULL] n_eval=5046  total_cost=43131.00  avg_cost=8.5476\n",
      "[15:17:39] → starte FIT mit Parametern: {'n_estimators': 16, 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 224}\n",
      "[15:17:43] ✓ FIT fertig\n",
      "[15:17:44] [rung 2/4 FULL] n_eval=5046  total_cost=48808.00  avg_cost=9.6726\n",
      "[15:17:45] → starte FIT mit Parametern: {'n_estimators': 32, 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 224}\n",
      "[15:17:52] ✓ FIT fertig\n",
      "[15:17:55] [rung 3/4 FULL] n_eval=5046  total_cost=50294.00  avg_cost=9.9671\n",
      "[15:17:56] → starte FIT mit Parametern: {'n_estimators': 64, 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 224}\n",
      "[15:18:10] ✓ FIT fertig\n",
      "[15:18:14] [rung 4/4 FULL] n_eval=5046  total_cost=49222.00  avg_cost=9.7547\n",
      "[I 2025-09-09 15:18:15,445] Trial 1 finished with value: 43131.0 and parameters: {'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 224}. Best is trial 1 with value: 43131.0.\n",
      "[Trial 001] state=COMPLETE value=43131.0000 best=43131.0000\n",
      "####################\n",
      "[15:18:15] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 224, 'min_samples_split': 112}\n",
      "[15:18:18] ✓ FIT fertig\n",
      "[15:18:19] [rung 1/4 FULL] n_eval=5046  total_cost=47176.00  avg_cost=9.3492\n",
      "[I 2025-09-09 15:18:20,248] Trial 2 pruned. \n",
      "[Trial 002] state=PRUNED value=47176.0000 best=43131.0000\n",
      "####################\n",
      "[15:18:20] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 224, 'min_samples_split': 112}\n",
      "[15:18:23] ✓ FIT fertig\n",
      "[15:18:24] [rung 1/4 FULL] n_eval=5046  total_cost=47176.00  avg_cost=9.3492\n",
      "[I 2025-09-09 15:18:25,501] Trial 3 pruned. \n",
      "[Trial 003] state=PRUNED value=47176.0000 best=43131.0000\n",
      "####################\n",
      "[15:18:26] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 56, 'min_samples_split': 112}\n",
      "[15:18:34] ✓ FIT fertig\n",
      "[15:18:35] [rung 1/4 FULL] n_eval=5046  total_cost=50166.00  avg_cost=9.9417\n",
      "[I 2025-09-09 15:18:36,749] Trial 4 pruned. \n",
      "[Trial 004] state=PRUNED value=50166.0000 best=43131.0000\n",
      "####################\n",
      "[15:18:37] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 112, 'min_samples_split': 448}\n",
      "[15:18:44] ✓ FIT fertig\n",
      "[15:18:45] [rung 1/4 FULL] n_eval=5046  total_cost=46864.00  avg_cost=9.2874\n",
      "[I 2025-09-09 15:18:46,373] Trial 5 pruned. \n",
      "[Trial 005] state=PRUNED value=46864.0000 best=43131.0000\n",
      "####################\n",
      "[15:18:46] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 0.4, 'min_samples_leaf': 112, 'min_samples_split': 448}\n",
      "[15:18:53] ✓ FIT fertig\n",
      "[15:18:54] [rung 1/4 FULL] n_eval=5046  total_cost=46864.00  avg_cost=9.2874\n",
      "[I 2025-09-09 15:18:55,456] Trial 6 pruned. \n",
      "[Trial 006] state=PRUNED value=46864.0000 best=43131.0000\n",
      "####################\n",
      "[15:18:56] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 448}\n",
      "[15:18:59] ✓ FIT fertig\n",
      "[15:19:00] [rung 1/4 FULL] n_eval=5046  total_cost=50515.00  avg_cost=10.0109\n",
      "[I 2025-09-09 15:19:01,635] Trial 7 pruned. \n",
      "[Trial 007] state=PRUNED value=50515.0000 best=43131.0000\n",
      "####################\n",
      "[SKIP] Trial 8 übersprungen – bekannte Param-Kombi: {'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 112, 'min_samples_split': 448}\n",
      "[I 2025-09-09 15:19:01,826] Trial 8 pruned. \n",
      "[15:19:02] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 224}\n",
      "[15:19:05] ✓ FIT fertig\n",
      "[15:19:06] [rung 1/4 FULL] n_eval=5046  total_cost=43666.00  avg_cost=8.6536\n",
      "[I 2025-09-09 15:19:07,502] Trial 9 pruned. \n",
      "[Trial 009] state=PRUNED value=43666.0000 best=43131.0000\n",
      "####################\n",
      "[15:19:08] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 112, 'min_samples_split': 224}\n",
      "[15:19:15] ✓ FIT fertig\n",
      "[15:19:16] [rung 1/4 FULL] n_eval=5046  total_cost=48311.00  avg_cost=9.5741\n",
      "[I 2025-09-09 15:19:19,558] Trial 10 pruned. \n",
      "[Trial 010] state=PRUNED value=48311.0000 best=43131.0000\n",
      "####################\n",
      "[15:19:21] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 112, 'min_samples_split': 112}\n",
      "[15:19:28] ✓ FIT fertig\n",
      "[15:19:29] [rung 1/4 FULL] n_eval=5046  total_cost=48311.00  avg_cost=9.5741\n",
      "[I 2025-09-09 15:19:30,359] Trial 11 pruned. \n",
      "[Trial 011] state=PRUNED value=48311.0000 best=43131.0000\n",
      "####################\n",
      "[15:19:30] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 0.4, 'min_samples_leaf': 56, 'min_samples_split': 224}\n",
      "[15:19:39] ✓ FIT fertig\n",
      "[15:19:39] [rung 1/4 FULL] n_eval=5046  total_cost=48734.00  avg_cost=9.6579\n",
      "[I 2025-09-09 15:19:40,673] Trial 12 pruned. \n",
      "[Trial 012] state=PRUNED value=48734.0000 best=43131.0000\n",
      "####################\n",
      "[15:19:41] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 0.4, 'min_samples_leaf': 56, 'min_samples_split': 448}\n",
      "[15:19:49] ✓ FIT fertig\n",
      "[15:19:50] [rung 1/4 FULL] n_eval=5046  total_cost=47723.00  avg_cost=9.4576\n",
      "[I 2025-09-09 15:19:50,937] Trial 13 pruned. \n",
      "[Trial 013] state=PRUNED value=47723.0000 best=43131.0000\n",
      "####################\n",
      "[15:19:51] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 0.4, 'min_samples_leaf': 112, 'min_samples_split': 224}\n",
      "[15:19:58] ✓ FIT fertig\n",
      "[15:19:59] [rung 1/4 FULL] n_eval=5046  total_cost=48311.00  avg_cost=9.5741\n",
      "[I 2025-09-09 15:19:59,970] Trial 14 pruned. \n",
      "[Trial 014] state=PRUNED value=48311.0000 best=43131.0000\n",
      "####################\n",
      "[15:20:00] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 224, 'min_samples_split': 448}\n",
      "[15:20:03] ✓ FIT fertig\n",
      "[15:20:04] [rung 1/4 FULL] n_eval=5046  total_cost=47176.00  avg_cost=9.3492\n",
      "[I 2025-09-09 15:20:04,985] Trial 15 pruned. \n",
      "[Trial 015] state=PRUNED value=47176.0000 best=43131.0000\n",
      "####################\n",
      "[SKIP] Trial 16 übersprungen – bekannte Param-Kombi: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 224, 'min_samples_split': 112}\n",
      "[I 2025-09-09 15:20:05,165] Trial 16 pruned. \n",
      "[15:20:05] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 56, 'min_samples_split': 448}\n",
      "[15:20:10] ✓ FIT fertig\n",
      "[15:20:11] [rung 1/4 FULL] n_eval=5046  total_cost=45591.00  avg_cost=9.0351\n",
      "[I 2025-09-09 15:20:12,288] Trial 17 pruned. \n",
      "[Trial 017] state=PRUNED value=45591.0000 best=43131.0000\n",
      "####################\n",
      "[SKIP] Trial 18 übersprungen – bekannte Param-Kombi: {'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 112, 'min_samples_split': 224}\n",
      "[I 2025-09-09 15:20:12,462] Trial 18 pruned. \n",
      "[15:20:12] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 0.4, 'min_samples_leaf': 112, 'min_samples_split': 112}\n",
      "[15:20:19] ✓ FIT fertig\n",
      "[15:20:20] [rung 1/4 FULL] n_eval=5046  total_cost=48311.00  avg_cost=9.5741\n",
      "[I 2025-09-09 15:20:21,363] Trial 19 pruned. \n",
      "[Trial 019] state=PRUNED value=48311.0000 best=43131.0000\n",
      "####################\n",
      "[15:20:21] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 112}\n",
      "[15:20:25] ✓ FIT fertig\n",
      "[15:20:25] [rung 1/4 FULL] n_eval=5046  total_cost=43131.00  avg_cost=8.5476\n",
      "[15:20:26] → starte FIT mit Parametern: {'n_estimators': 16, 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 112}\n",
      "[15:20:31] ✓ FIT fertig\n",
      "[15:20:32] [rung 2/4 FULL] n_eval=5046  total_cost=48808.00  avg_cost=9.6726\n",
      "[15:20:33] → starte FIT mit Parametern: {'n_estimators': 32, 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 112}\n",
      "[15:20:40] ✓ FIT fertig\n",
      "[15:20:42] [rung 3/4 FULL] n_eval=5046  total_cost=50294.00  avg_cost=9.9671\n",
      "[15:20:43] → starte FIT mit Parametern: {'n_estimators': 64, 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 112}\n",
      "[15:20:56] ✓ FIT fertig\n",
      "[15:20:59] [rung 4/4 FULL] n_eval=5046  total_cost=49222.00  avg_cost=9.7547\n",
      "[I 2025-09-09 15:21:00,014] Trial 20 finished with value: 43131.0 and parameters: {'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 112}. Best is trial 1 with value: 43131.0.\n",
      "[Trial 020] state=COMPLETE value=43131.0000 best=43131.0000\n",
      "####################\n",
      "[15:21:00] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 56, 'min_samples_split': 448}\n",
      "[15:21:08] ✓ FIT fertig\n",
      "[15:21:09] [rung 1/4 FULL] n_eval=5046  total_cost=47980.00  avg_cost=9.5085\n",
      "[I 2025-09-09 15:21:10,620] Trial 21 pruned. \n",
      "[Trial 021] state=PRUNED value=47980.0000 best=43131.0000\n",
      "####################\n",
      "[15:21:11] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 0.4, 'min_samples_leaf': 56, 'min_samples_split': 112}\n",
      "[15:21:19] ✓ FIT fertig\n",
      "[15:21:20] [rung 1/4 FULL] n_eval=5046  total_cost=46260.00  avg_cost=9.1677\n",
      "[I 2025-09-09 15:21:24,168] Trial 22 pruned. \n",
      "[Trial 022] state=PRUNED value=46260.0000 best=43131.0000\n",
      "####################\n",
      "[SKIP] Trial 23 übersprungen – bekannte Param-Kombi: {'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 224, 'min_samples_split': 112}\n",
      "[I 2025-09-09 15:21:24,874] Trial 23 pruned. \n",
      "[SKIP] Trial 24 übersprungen – bekannte Param-Kombi: {'max_depth': 12, 'max_features': 0.4, 'min_samples_leaf': 56, 'min_samples_split': 224}\n",
      "[I 2025-09-09 15:21:24,876] Trial 24 pruned. \n",
      "[15:21:26] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 56, 'min_samples_split': 224}\n",
      "[15:21:35] ✓ FIT fertig\n",
      "[15:21:35] [rung 1/4 FULL] n_eval=5046  total_cost=50642.00  avg_cost=10.0361\n",
      "[I 2025-09-09 15:21:36,866] Trial 25 pruned. \n",
      "[Trial 025] state=PRUNED value=50642.0000 best=43131.0000\n",
      "####################\n",
      "[15:21:37] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 0.4, 'min_samples_leaf': 224, 'min_samples_split': 112}\n",
      "[15:21:43] ✓ FIT fertig\n",
      "[15:21:43] [rung 1/4 FULL] n_eval=5046  total_cost=48284.00  avg_cost=9.5688\n",
      "[I 2025-09-09 15:21:44,747] Trial 26 pruned. \n",
      "[Trial 026] state=PRUNED value=48284.0000 best=43131.0000\n",
      "####################\n",
      "[15:21:45] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 224, 'min_samples_split': 448}\n",
      "[15:21:50] ✓ FIT fertig\n",
      "[15:21:51] [rung 1/4 FULL] n_eval=5046  total_cost=48284.00  avg_cost=9.5688\n",
      "[I 2025-09-09 15:21:52,229] Trial 27 pruned. \n",
      "[Trial 027] state=PRUNED value=48284.0000 best=43131.0000\n",
      "####################\n",
      "[15:21:52] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 224, 'min_samples_split': 224}\n",
      "[15:21:55] ✓ FIT fertig\n",
      "[15:21:56] [rung 1/4 FULL] n_eval=5046  total_cost=47176.00  avg_cost=9.3492\n",
      "[I 2025-09-09 15:21:57,128] Trial 28 pruned. \n",
      "[Trial 028] state=PRUNED value=47176.0000 best=43131.0000\n",
      "####################\n",
      "[SKIP] Trial 29 übersprungen – bekannte Param-Kombi: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 224}\n",
      "[I 2025-09-09 15:21:57,300] Trial 29 pruned. \n",
      "[15:21:57] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 0.4, 'min_samples_leaf': 224, 'min_samples_split': 448}\n",
      "[15:22:03] ✓ FIT fertig\n",
      "[15:22:03] [rung 1/4 FULL] n_eval=5046  total_cost=48284.00  avg_cost=9.5688\n",
      "[I 2025-09-09 15:22:04,645] Trial 30 pruned. \n",
      "[Trial 030] state=PRUNED value=48284.0000 best=43131.0000\n",
      "####################\n",
      "[SKIP] Trial 31 übersprungen – bekannte Param-Kombi: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 224, 'min_samples_split': 112}\n",
      "[I 2025-09-09 15:22:04,815] Trial 31 pruned. \n",
      "[SKIP] Trial 32 übersprungen – bekannte Param-Kombi: {'max_depth': 16, 'max_features': 0.4, 'min_samples_leaf': 224, 'min_samples_split': 448}\n",
      "[I 2025-09-09 15:22:04,817] Trial 32 pruned. \n",
      "[15:22:05] → starte FIT mit Parametern: {'n_estimators': 8, 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 56, 'min_samples_split': 112}\n",
      "[15:22:09] ✓ FIT fertig\n",
      "[15:22:09] [rung 1/4 FULL] n_eval=5046  total_cost=50296.00  avg_cost=9.9675\n",
      "[I 2025-09-09 15:22:10,549] Trial 33 pruned. \n",
      "[Trial 033] state=PRUNED value=50296.0000 best=43131.0000\n",
      "####################\n",
      "[SKIP] Trial 34 übersprungen – bekannte Param-Kombi: {'max_depth': 12, 'max_features': 0.4, 'min_samples_leaf': 224, 'min_samples_split': 112}\n",
      "[I 2025-09-09 15:22:10,720] Trial 34 pruned. \n",
      "[SKIP] Trial 35 übersprungen – bekannte Param-Kombi: {'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 56, 'min_samples_split': 448}\n",
      "[I 2025-09-09 15:22:10,722] Trial 35 pruned. \n",
      "Beste Parameter nach realisierten Gesamtkosten: {'max_depth': 16, 'max_features': 'sqrt', 'min_samples_leaf': 112, 'min_samples_split': 224, 'n_estimators': 8}\n",
      "Beste Gesamtkosten: 43131.0\n"
     ]
    }
   ],
   "source": [
    "# y_train_surv bereits als structured array vorhanden\n",
    "study = run_rsf_study_totalcost(\n",
    "    X_train=X_train,\n",
    "    y_train_surv=y_train_surv,\n",
    "    X_val=X_val,\n",
    "    y_val_class=validation_data[\"class_label\"],   # deine vorhandenen Klassenlabels\n",
    "    experiment_name=\"RSF_HPO_TOTALCOST_test\"\n",
    ")\n",
    "\n",
    "best_params = study.best_trial.user_attrs[\"full_params\"]\n",
    "print(\"Beste Parameter nach realisierten Gesamtkosten:\", best_params)\n",
    "print(\"Beste Gesamtkosten:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(pd.DataFrame([best_params]), ordner=\"05_model_input\", name=\"rsf_best_params_totalcost_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d6a9b",
   "metadata": {},
   "source": [
    "### 2. XGBoosting mit AFT HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d490e9",
   "metadata": {},
   "source": [
    "#### Daten vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f5d01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rsf_model_input(df: pd.DataFrame, columns_to_drop) -> pd.DataFrame: \n",
    "    \"\"\" Prepares the input data for the XGBoost model with aft. \n",
    "    \n",
    "    Args: \n",
    "    df (pd.DataFrame): The input dataframe containing features and target variables. \n",
    "    columns_to_drop (list): List of columns to drop from the dataframe. \n",
    "\n",
    "    Return:\n",
    "    pd.DataFrame: The feature dataframe for XGBoost. \n",
    "    \"\"\"\n",
    "\n",
    "    y = {\n",
    "        \"lower_bound\": df[\"duration\"].astype(float),\n",
    "        \"upper_bound\":  df[\"upper_bound\"].astype(float),\n",
    "    }\n",
    "\n",
    "    x = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    d = xgb.DMatrix(data=x, label_lower_bound=y[\"lower_bound\"],\n",
    "                     label_upper_bound=y[\"upper_bound\"])\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2005e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = prepare_rsf_model_input(load_df(ordner=\"04_feature\", name = \"feature_train_corr_labels\"), columns_to_drop=[\"duration\", \"event\", \"vehicle_id\", \"class\", \"upper_bound\"])\n",
    "dval   = prepare_rsf_model_input(load_df(ordner=\"04_feature\", name = \"feature_validation_corr_labels\"), columns_to_drop=[\"duration\", \"event\", \"vehicle_id\", \"class_label\", \"upper_bound\"])\n",
    "labels_val = load_df(ordner=\"04_feature\", name = \"feature_validation_corr_labels\")[\"class_label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/05_model_input\", exist_ok=True)\n",
    "\n",
    "xdtrain.save_binary(\"../data/05_model_input/AFT/xdtrain.buffer\")\n",
    "xdval.save_binary(\"../data/05_model_input/AFT/xdval.buffer\")\n",
    "\n",
    "# Label separat als NumPy/Parquet speichern\n",
    "label_val.to_frame(\"class_label\").to_parquet(\"../data/05_model_input/AFT/label_val.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f72cbc",
   "metadata": {},
   "source": [
    "#### Loss + Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c85a0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_aft_params(trial: optuna.Trial) -> dict:\n",
    "    \"\"\"\n",
    "    Builds hyperparameters for XGBoost AFT model using Optuna trial suggestions.\n",
    "\n",
    "    Args:\n",
    "        trial: optuna.Trial object for hyperparameter suggestions.\n",
    "    \n",
    "    Return:\n",
    "        dict: Hyperparameters for XGBoost AFT model.\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"survival:aft\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"eval_metric\": \"aft-nloglik\",\n",
    "\n",
    "        # Modellkapazität\n",
    "        \"eta\": trial.suggest_float(\"eta\", 0.01, 0.2, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 14),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 0.1, 100.0, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "\n",
    "        # Regularisierung\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-4, 100.0, log=True),\n",
    "        \"alpha\":  trial.suggest_float(\"alpha\",  1e-4, 100.0, log=True),\n",
    "\n",
    "        # Subsampling & Spalten\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "\n",
    "        # Hist/Sampling\n",
    "        \"max_bin\": trial.suggest_categorical(\"max_bin\", [64, 128, 256, 512]),\n",
    "        \"sampling_method\": trial.suggest_categorical(\"sampling_method\", [\"uniform\", \"gradient_based\"]),\n",
    "\n",
    "        # AFT-spezifisch\n",
    "        \"aft_loss_distribution\": trial.suggest_categorical(\n",
    "            \"aft_loss_distribution\", [\"normal\", \"logistic\", \"extreme\"]\n",
    "        ),\n",
    "        \"aft_loss_distribution_scale\": trial.suggest_float(\n",
    "            \"aft_loss_distribution_scale\", 0.2, 10.0, log=True\n",
    "        ),\n",
    "\n",
    "        \"verbosity\": 0,\n",
    "    }\n",
    "\n",
    "    # Wachstumsstrategie\n",
    "    params[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    if params[\"grow_policy\"] == \"lossguide\":\n",
    "        params[\"max_leaves\"] = trial.suggest_int(\"max_leaves\", 16, 512, log=True)\n",
    "\n",
    "    # Booster-Variante (optional)\n",
    "    params[\"booster\"] = trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"])\n",
    "    if params[\"booster\"] == \"dart\":\n",
    "        params.update({\n",
    "            \"rate_drop\": trial.suggest_float(\"rate_drop\", 0.0, 0.5),\n",
    "            \"skip_drop\": trial.suggest_float(\"skip_drop\", 0.0, 0.1),\n",
    "            \"normalize_type\": trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"]),\n",
    "            \"sample_type\": trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"]),\n",
    "        })\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8951b5",
   "metadata": {},
   "source": [
    "#### Evaluation über erwartete Kosten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d26adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_survival_prob(dval: xgb.DMatrix, booster: xgb.Booster, sigma: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predicts survival probabilities at specified time points using an XGBoost AFT model.\n",
    "\n",
    "    Args:\n",
    "        dval: xgb.DMatrix containing validation data.\n",
    "        booster: Trained XGBoost Booster model.\n",
    "        sigma: Scale parameter for the normal distribution.\n",
    "    \n",
    "    Return:\n",
    "        np.ndarray: Survival probabilities at specified time points.\n",
    "    \"\"\"\n",
    "\n",
    "    mu = booster.predict(dval)  # (n_samples, 2): mean + std\n",
    "    sigma = max(sigma, 1e-3)\n",
    "\n",
    "    # Survivalfunktion S(t) für alle TAUS auswerten\n",
    "    S = np.stack([\n",
    "        1.0 - scipy.stats.norm.cdf(tau, loc=mu, scale=sigma)\n",
    "        for tau in TAUS\n",
    "    ], axis=1)\n",
    "\n",
    "    return S  # shape: (n_samples, len(TAUS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaed7cd",
   "metadata": {},
   "source": [
    "#### Optuna Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec21cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aft_objective(trial: optuna.Trial,\n",
    "                  y_val_class: pd.Series,\n",
    "                  dtrain: xgb.DMatrix, dval: xgb.DMatrix,\n",
    "                  experiment_name: str = \"AFT_HPO_TOTALCOST\"\n",
    "                  ) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize XGBoost AFT model hyperparameters\n",
    "\n",
    "    Args:\n",
    "        trial: optuna.Trial object for hyperparameter suggestions.\n",
    "        y_val_class: True class labels for validation data.\n",
    "        dtrain: xgb.DMatrix containing training data.\n",
    "        dval: xgb.DMatrix containing validation data.\n",
    "        experiment_name: Name of the MLflow experiment.\n",
    "    \n",
    "    Return:\n",
    "        float: The total expected cost on the validation set.\n",
    "    \"\"\"\n",
    "\n",
    "    params = build_aft_params(trial)\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"eval\")],\n",
    "            num_boost_round=trial.suggest_int(\"n_estimators\", 50, 1000, step=50),\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        S_tau = predict_survival_prob(dval, booster, sigma=params[\"aft_loss_distribution_scale\"])\n",
    "        total_cost = total_expected_cost_at_surv(S_tau, TAUS, y_val_class, COST)\n",
    "        mlflow.log_metric(\"val_total_expected_cost\", total_cost)\n",
    "        mlflow.log_metric(\"num_boost_round\", booster.best_iteration)\n",
    "\n",
    "        return total_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e40bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aft_objective_acc(trial: optuna.Trial,\n",
    "                  y_val_class: pd.Series,\n",
    "                  dtrain: xgb.DMatrix, dval: xgb.DMatrix,\n",
    "                  experiment_name: str = \"AFT_HPO_TOTALCOST\"\n",
    "                  ) -> float:\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize XGBoost AFT model hyperparameters\n",
    "\n",
    "    Args:\n",
    "        trial: optuna.Trial object for hyperparameter suggestions.\n",
    "        y_val_class: True class labels for validation data.\n",
    "        dtrain: xgb.DMatrix containing training data.\n",
    "        dval: xgb.DMatrix containing validation data.\n",
    "        experiment_name: Name of the MLflow experiment.\n",
    "    \n",
    "    Return:\n",
    "        float: Val AFT Balanced Accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    params = build_aft_params(trial)\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"eval\")],\n",
    "            num_boost_round=trial.suggest_int(\"n_estimators\", 50, 1000, step=50),\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        S_tau = predict_survival_prob(dval, booster, sigma=params[\"aft_loss_distribution_scale\"])\n",
    "        total_cost = total_expected_cost_at_surv(S_tau, TAUS, y_val_class, COST)\n",
    "        # Accuracy\n",
    "        probs = np.vstack([class_probs_from_S_tau(s) for s in S_tau])\n",
    "        pred_class = probs.argmax(axis=1)\n",
    "        acc = accuracy_score(y_val_class, pred_class)\n",
    "        acc_balanced = balanced_accuracy_score(y_val_class, pred_class)\n",
    "        mlflow.log_metric(\"val_total_expected_cost\", total_cost)\n",
    "        mlflow.log_metric(\"num_boost_round\", booster.best_iteration)\n",
    "        mlflow.log_metric(\"val_balanced_accuracy\", acc_balanced)\n",
    "        mlflow.log_metric(\"val_accuracy\", acc)\n",
    "        log(f\"Val AFT Total Expected Cost: {total_cost}\")\n",
    "        log(f\"Val AFT Balanced Accuracy: {acc_balanced}\")\n",
    "        log(f\"Val AFT Accuracy: {acc}\")\n",
    "\n",
    "        return acc_balanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f306199",
   "metadata": {},
   "source": [
    "#### Cost Funktion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c438bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_expected_cost_at_surv(S_tau: np.ndarray, taus: np.ndarray,\n",
    "                               true_classes: np.ndarray, cost: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Berechnet die realisierten Gesamtkosten Cost[n, m̂] auf Basis der Über\n",
    "    lebensfunktionen S(tau1..tau4).\n",
    "\n",
    "    Args:\n",
    "        S_tau: Überlebensfunktionen an den Klassengrenzen, Form (N, 4)\n",
    "        taus: Klassengrenzen der Form (4,)\n",
    "        true_classes: Wahre Klassen der Form (N,)\n",
    "        cost: Kostenmatrix der Form (5,5)\n",
    "    \n",
    "    Return:\n",
    "        float: Die realisierten Gesamtkosten.\n",
    "    \"\"\"\n",
    "    total_cost = 0.0\n",
    "    for i, s in enumerate(S_tau):\n",
    "        p = class_probs_from_S_tau(s)\n",
    "        exp_cost = cost.T @ p\n",
    "        m_hat = int(np.argmin(exp_cost))\n",
    "        total_cost += cost[int(true_classes[i]), m_hat]\n",
    "    return total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a819f9",
   "metadata": {},
   "source": [
    "#### Anwendung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f031c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-08 16:58:22,322] A new study created in memory with name: no-name-d787c649-96f4-4f80-bcda-9be26aa11d0d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dced2365e1042e7a9cf87e839de8202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 16:58:22 INFO mlflow.tracking.fluent: Experiment with name 'AFT_HPO_TOTALCOST' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-09-08 16:58:26,326] Trial 0 failed with parameters: {'aft_loss_distribution_scale': 1.3267901727825773, 'eta': 0.11394375197074692, 'max_depth': 6, 'min_child_weight': 23.340490980843327, 'lambda': 1.1740181831622858, 'alpha': 0.0021045289001949525, 'subsample': 0.68892903206879, 'colsample_bytree': 0.9554128443468315, 'booster': 'dart'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1089/691840537.py\", line 4, in <lambda>\n",
      "    study.optimize(lambda trial: aft_objective(trial, y_val_class=validation_data[\"class_label\"], dtrain=dtrain, dval=dval),\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1089/3933472990.py\", line 11, in aft_objective\n",
      "    mlflow.log_params(params)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mlflow/tracking/fluent.py\", line 1167, in log_params\n",
      "    return MlflowClient().log_batch(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mlflow/tracking/client.py\", line 2378, in log_batch\n",
      "    return self._tracking_client.log_batch(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mlflow/tracking/_tracking_service/client.py\", line 531, in log_batch\n",
      "    self.store.log_batch(\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mlflow/store/tracking/file_store.py\", line 1178, in log_batch\n",
      "    self._log_run_param(run_info, param)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mlflow/store/tracking/file_store.py\", line 1077, in _log_run_param\n",
      "    param_path = self._get_param_path(run_info.experiment_id, run_info.run_id, param.key)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mlflow/store/tracking/file_store.py\", line 279, in _get_param_path\n",
      "    self._get_run_dir(experiment_id, run_uuid),\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mlflow/store/tracking/file_store.py\", line 258, in _get_run_dir\n",
      "    return os.path.join(self._get_experiment_path(experiment_id, assert_exists=True), run_uuid)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mlflow/store/tracking/file_store.py\", line 244, in _get_experiment_path\n",
      "    exp_list = find(parent, experiment_id, full_path=True)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mlflow/utils/file_utils.py\", line 191, in find\n",
      "    return list_all(root, lambda x: x == path_name, full_path)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/mlflow/utils/file_utils.py\", line 144, in list_all\n",
      "    matches = [x for x in os.listdir(root) if filter_func(os.path.join(root, x))]\n",
      "                          ^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-09-08 16:58:26,329] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maft_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m               \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m               \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_callback_totalcost\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m               \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Logge das beste Ergebnis\u001b[39;00m\n\u001b[1;32m     10\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py:258\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    254\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    257\u001b[0m ):\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/optuna/study/_optimize.py:201\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[43maft_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdval\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      5\u001b[0m                n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      6\u001b[0m                callbacks\u001b[38;5;241m=\u001b[39m[progress_callback_totalcost],\n\u001b[1;32m      7\u001b[0m                show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Logge das beste Ergebnis\u001b[39;00m\n\u001b[1;32m     10\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\n",
      "Cell \u001b[0;32mIn[31], line 11\u001b[0m, in \u001b[0;36maft_objective\u001b[0;34m(trial, y_val_class, dtrain, dval, experiment_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mset_experiment(experiment_name)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 11\u001b[0m     \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     booster \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m     14\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m     15\u001b[0m         dtrain\u001b[38;5;241m=\u001b[39mdtrain,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m         verbose_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     S_tau \u001b[38;5;241m=\u001b[39m predict_survival_prob(dval, booster, sigma\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maft_loss_distribution_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/tracking/fluent.py:1167\u001b[0m, in \u001b[0;36mlog_params\u001b[0;34m(params, synchronous, run_id)\u001b[0m\n\u001b[1;32m   1165\u001b[0m params_arr \u001b[38;5;241m=\u001b[39m [Param(key, \u001b[38;5;28mstr\u001b[39m(value)) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   1166\u001b[0m synchronous \u001b[38;5;241m=\u001b[39m synchronous \u001b[38;5;28;01mif\u001b[39;00m synchronous \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m MLFLOW_ENABLE_ASYNC_LOGGING\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m-> 1167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMlflowClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronous\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/tracking/client.py:2378\u001b[0m, in \u001b[0;36mMlflowClient.log_batch\u001b[0;34m(self, run_id, metrics, params, tags, synchronous)\u001b[0m\n\u001b[1;32m   2375\u001b[0m \u001b[38;5;66;03m# Stringify the values of the params\u001b[39;00m\n\u001b[1;32m   2376\u001b[0m params \u001b[38;5;241m=\u001b[39m [Param(key\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mkey, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(param\u001b[38;5;241m.\u001b[39mvalue)) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m params]\n\u001b[0;32m-> 2378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronous\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/tracking/_tracking_service/client.py:531\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_batch\u001b[0;34m(self, run_id, metrics, params, tags, synchronous)\u001b[0m\n\u001b[1;32m    528\u001b[0m metrics \u001b[38;5;241m=\u001b[39m metrics[metrics_batch_size:]\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synchronous:\n\u001b[0;32m--> 531\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags_batch\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m     run_operations_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mlog_batch_async(\n\u001b[1;32m    537\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mrun_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         )\n\u001b[1;32m    542\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/store/tracking/file_store.py:1178\u001b[0m, in \u001b[0;36mFileStore.log_batch\u001b[0;34m(self, run_id, metrics, params, tags)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m-> 1178\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_run_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_run_metric(run_info, metric)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/store/tracking/file_store.py:1077\u001b[0m, in \u001b[0;36mFileStore._log_run_param\u001b[0;34m(self, run_info, param)\u001b[0m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_log_run_param\u001b[39m(\u001b[38;5;28mself\u001b[39m, run_info, param):\n\u001b[0;32m-> 1077\u001b[0m     param_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_param_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1078\u001b[0m     writeable_param_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writeable_value(param\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(param_path):\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/store/tracking/file_store.py:279\u001b[0m, in \u001b[0;36mFileStore._get_param_path\u001b[0;34m(self, experiment_id, run_uuid, param_name)\u001b[0m\n\u001b[1;32m    276\u001b[0m _validate_run_id(run_uuid)\n\u001b[1;32m    277\u001b[0m _validate_param_name(param_name)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_run_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_uuid\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    280\u001b[0m     FileStore\u001b[38;5;241m.\u001b[39mPARAMS_FOLDER_NAME,\n\u001b[1;32m    281\u001b[0m     param_name,\n\u001b[1;32m    282\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/store/tracking/file_store.py:258\u001b[0m, in \u001b[0;36mFileStore._get_run_dir\u001b[0;34m(self, experiment_id, run_uuid)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_experiment(experiment_id):\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_experiment_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massert_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m, run_uuid)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/store/tracking/file_store.py:244\u001b[0m, in \u001b[0;36mFileStore._get_experiment_path\u001b[0;34m(self, experiment_id, view_type, assert_exists)\u001b[0m\n\u001b[1;32m    242\u001b[0m     parents\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrash_folder)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parent \u001b[38;5;129;01min\u001b[39;00m parents:\n\u001b[0;32m--> 244\u001b[0m     exp_list \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exp_list) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m exp_list[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/utils/file_utils.py:191\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(root, name, full_path)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Search for a file in a root directory. Equivalent to:\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;124;03m  ``find $root -name \"$name\" -depth 1``\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m    list of matching files or directories.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    190\u001b[0m path_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, name)\n\u001b[0;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlist_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpath_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/utils/file_utils.py:144\u001b[0m, in \u001b[0;36mlist_all\u001b[0;34m(root, filter_func, full_path)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(root):\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid parent directory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m matches \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m filter_func(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, x))]\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m matches] \u001b[38;5;28;01mif\u001b[39;00m full_path \u001b[38;5;28;01melse\u001b[39;00m matches\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "\n",
    "study.optimize(lambda trial: aft_objective(trial, y_val_class=labels_val, dtrain=dtrain, dval=dval),\n",
    "               n_trials=1000,\n",
    "               callbacks=[progress_callback_totalcost],\n",
    "               show_progress_bar=True)\n",
    "\n",
    "# Logge das beste Ergebnis\n",
    "best_trial = study.best_trial\n",
    "best_params = best_trial.params\n",
    "mlflow.log_params(best_trial.params)\n",
    "mlflow.log_metric(\"best_val_total_expected_cost\", best_trial.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "540f25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1571cf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter nach realisierten Gesamtkosten: {'aft_loss_distribution': 'logistic', 'aft_loss_distribution_scale': 0.5, 'learning_rate': 0.28708886146175144, 'max_depth': 14, 'min_child_weight': 96, 'lambda': 0.00028905635749732183, 'alpha': 0.020669977889093343, 'subsample': 0.9859106682012534, 'colsample_bytree': 0.7390509265179644, 'n_estimators': 450}\n"
     ]
    }
   ],
   "source": [
    "print(\"Beste Parameter nach realisierten Gesamtkosten:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "da78b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(pd.DataFrame([best_params]), ordner=\"05_model_input\", name=\"atf_best_params_totalcost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365dc879",
   "metadata": {},
   "source": [
    "#### Anwendung beasierend auf acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-10 09:30:44,292] A new study created in memory with name: no-name-8543f209-ad4a-4714-8fbd-4c332191e257\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f64acf0c6a6409c9bb813f6086c98d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:30:58] Val AFT Total Expected Cost: 57430.0\n",
      "[09:30:58] Val AFT Balanced Accuracy: 0.19983706720977595\n",
      "[09:30:58] Val AFT Accuracy: 0.9722552516845026\n",
      "[I 2025-09-10 09:30:58,773] Trial 0 finished with value: 0.19983706720977595 and parameters: {'eta': 0.03689476470418179, 'max_depth': 12, 'min_child_weight': 0.6136930669391633, 'gamma': 1.5143816440193185, 'lambda': 0.04409269526700462, 'alpha': 75.25409538918642, 'subsample': 0.7987398535759834, 'colsample_bytree': 0.5656820354059564, 'colsample_bylevel': 0.7806102392547596, 'max_bin': 64, 'sampling_method': 'uniform', 'aft_loss_distribution': 'logistic', 'aft_loss_distribution_scale': 1.0898843549441344, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'n_estimators': 1000}. Best is trial 0 with value: 0.19983706720977595.\n",
      "[Trial 000] state=COMPLETE value=0.1998 best=0.1998\n",
      "####################\n",
      "[09:58:24] Val AFT Total Expected Cost: 49566.0\n",
      "[09:58:24] Val AFT Balanced Accuracy: 0.2\n",
      "[09:58:24] Val AFT Accuracy: 0.015061434799841459\n",
      "[I 2025-09-10 09:58:24,149] Trial 1 finished with value: 0.2 and parameters: {'eta': 0.05886205016563002, 'max_depth': 13, 'min_child_weight': 13.348913502873337, 'gamma': 1.5298038894171406, 'lambda': 0.025968108405661654, 'alpha': 1.4010014525547214, 'subsample': 0.964854917507892, 'colsample_bytree': 0.6170129707841172, 'colsample_bylevel': 0.6798694523446172, 'max_bin': 256, 'sampling_method': 'uniform', 'aft_loss_distribution': 'extreme', 'aft_loss_distribution_scale': 1.2011449058126806, 'grow_policy': 'lossguide', 'max_leaves': 419, 'booster': 'dart', 'rate_drop': 0.360720327005303, 'skip_drop': 0.004485436699403556, 'normalize_type': 'tree', 'sample_type': 'uniform', 'n_estimators': 800}. Best is trial 1 with value: 0.2.\n",
      "[Trial 001] state=COMPLETE value=0.2000 best=0.2000\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/scipy/stats/_distn_infrastructure.py:2069: RuntimeWarning: overflow encountered in divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:58:46] Val AFT Total Expected Cost: 59861.0\n",
      "[09:58:46] Val AFT Balanced Accuracy: 0.20122047736449064\n",
      "[09:58:46] Val AFT Accuracy: 0.8357114546175188\n",
      "[I 2025-09-10 09:58:46,534] Trial 2 finished with value: 0.20122047736449064 and parameters: {'eta': 0.04086771866645587, 'max_depth': 9, 'min_child_weight': 0.9706254600847153, 'gamma': 0.6117931128561122, 'lambda': 7.816344036833005, 'alpha': 2.9220580586684646, 'subsample': 0.8639033872250099, 'colsample_bytree': 0.666355653811576, 'colsample_bylevel': 0.8633266518489359, 'max_bin': 512, 'sampling_method': 'gradient_based', 'aft_loss_distribution': 'logistic', 'aft_loss_distribution_scale': 0.37349123145911867, 'grow_policy': 'depthwise', 'booster': 'dart', 'rate_drop': 0.17698519480437086, 'skip_drop': 0.07629626059468228, 'normalize_type': 'tree', 'sample_type': 'uniform', 'n_estimators': 200}. Best is trial 2 with value: 0.20122047736449064.\n",
      "[Trial 002] state=COMPLETE value=0.2012 best=0.2012\n",
      "####################\n",
      "[09:59:56] Val AFT Total Expected Cost: 57489.0\n",
      "[09:59:56] Val AFT Balanced Accuracy: 0.19906313645621182\n",
      "[09:59:56] Val AFT Accuracy: 0.9684898929845422\n",
      "[I 2025-09-10 09:59:56,251] Trial 3 finished with value: 0.19906313645621182 and parameters: {'eta': 0.13462535141018003, 'max_depth': 10, 'min_child_weight': 0.20554007965069945, 'gamma': 1.1177461290672626, 'lambda': 1.1104141307446689, 'alpha': 0.10953854088388347, 'subsample': 0.743944354909736, 'colsample_bytree': 0.7755282844897462, 'colsample_bylevel': 0.6371938892549244, 'max_bin': 128, 'sampling_method': 'uniform', 'aft_loss_distribution': 'normal', 'aft_loss_distribution_scale': 0.42956283790547084, 'grow_policy': 'lossguide', 'max_leaves': 312, 'booster': 'dart', 'rate_drop': 0.11096362791873193, 'skip_drop': 0.03899818494643853, 'normalize_type': 'forest', 'sample_type': 'weighted', 'n_estimators': 500}. Best is trial 2 with value: 0.20122047736449064.\n",
      "[Trial 003] state=COMPLETE value=0.1991 best=0.2012\n",
      "####################\n",
      "[10:01:58] Val AFT Total Expected Cost: 57321.0\n",
      "[10:01:58] Val AFT Balanced Accuracy: 0.19991853360488798\n",
      "[10:01:58] Val AFT Accuracy: 0.9726516052318668\n",
      "[I 2025-09-10 10:01:58,589] Trial 4 finished with value: 0.19991853360488798 and parameters: {'eta': 0.046379376691510335, 'max_depth': 10, 'min_child_weight': 0.7883117646107735, 'gamma': 3.7573646743807294, 'lambda': 51.94361007954831, 'alpha': 0.133637152196745, 'subsample': 0.8925001845237905, 'colsample_bytree': 0.7782114033165171, 'colsample_bylevel': 0.5499901990699798, 'max_bin': 512, 'sampling_method': 'gradient_based', 'aft_loss_distribution': 'normal', 'aft_loss_distribution_scale': 0.718772526676629, 'grow_policy': 'depthwise', 'booster': 'dart', 'rate_drop': 0.14251245747152963, 'skip_drop': 0.09870583969763574, 'normalize_type': 'forest', 'sample_type': 'uniform', 'n_estimators': 250}. Best is trial 2 with value: 0.20122047736449064.\n",
      "[Trial 004] state=COMPLETE value=0.1999 best=0.2012\n",
      "####################\n",
      "[10:02:03] Val AFT Total Expected Cost: 57400.0\n",
      "[10:02:03] Val AFT Balanced Accuracy: 0.2\n",
      "[10:02:03] Val AFT Accuracy: 0.9730479587792311\n",
      "[I 2025-09-10 10:02:03,414] Trial 5 finished with value: 0.2 and parameters: {'eta': 0.11194871374587224, 'max_depth': 7, 'min_child_weight': 1.463133040891962, 'gamma': 0.5475872611899901, 'lambda': 0.3022891310286148, 'alpha': 0.10442377786718814, 'subsample': 0.8053851806621777, 'colsample_bytree': 0.8627262665388029, 'colsample_bylevel': 0.8990773237838061, 'max_bin': 64, 'sampling_method': 'gradient_based', 'aft_loss_distribution': 'extreme', 'aft_loss_distribution_scale': 2.1190268086606006, 'grow_policy': 'lossguide', 'max_leaves': 52, 'booster': 'gbtree', 'n_estimators': 300}. Best is trial 2 with value: 0.20122047736449064.\n",
      "[Trial 005] state=COMPLETE value=0.2000 best=0.2012\n",
      "####################\n",
      "[10:02:29] Val AFT Total Expected Cost: 57400.0\n",
      "[10:02:29] Val AFT Balanced Accuracy: 0.2\n",
      "[10:02:29] Val AFT Accuracy: 0.9730479587792311\n",
      "[I 2025-09-10 10:02:29,911] Trial 6 finished with value: 0.2 and parameters: {'eta': 0.061656286219809675, 'max_depth': 5, 'min_child_weight': 51.38915747613228, 'gamma': 1.0193809595687946, 'lambda': 0.07787848856190248, 'alpha': 0.5259461520120772, 'subsample': 0.6316903261840505, 'colsample_bytree': 0.975002701920576, 'colsample_bylevel': 0.5894460598695661, 'max_bin': 128, 'sampling_method': 'uniform', 'aft_loss_distribution': 'logistic', 'aft_loss_distribution_scale': 1.4723993773374893, 'grow_policy': 'depthwise', 'booster': 'dart', 'rate_drop': 0.3415656072723593, 'skip_drop': 0.07206832156164499, 'normalize_type': 'forest', 'sample_type': 'uniform', 'n_estimators': 300}. Best is trial 2 with value: 0.20122047736449064.\n",
      "[Trial 006] state=COMPLETE value=0.2000 best=0.2012\n",
      "####################\n",
      "[10:02:36] Val AFT Total Expected Cost: 57260.0\n",
      "[10:02:36] Val AFT Balanced Accuracy: 0.1971325972773073\n",
      "[10:02:36] Val AFT Accuracy: 0.9464922711058263\n",
      "[I 2025-09-10 10:02:36,618] Trial 7 finished with value: 0.1971325972773073 and parameters: {'eta': 0.06223015498140471, 'max_depth': 7, 'min_child_weight': 22.187982892520054, 'gamma': 1.0133353750589413, 'lambda': 7.771711203451508, 'alpha': 0.8176923859570917, 'subsample': 0.7184773148784637, 'colsample_bytree': 0.8885328756042234, 'colsample_bylevel': 0.8750364995040636, 'max_bin': 512, 'sampling_method': 'uniform', 'aft_loss_distribution': 'logistic', 'aft_loss_distribution_scale': 0.7123140259877045, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'n_estimators': 400}. Best is trial 2 with value: 0.20122047736449064.\n",
      "[Trial 007] state=COMPLETE value=0.1971 best=0.2012\n",
      "####################\n",
      "[10:05:12] Val AFT Total Expected Cost: 57400.0\n",
      "[10:05:12] Val AFT Balanced Accuracy: 0.2\n",
      "[10:05:12] Val AFT Accuracy: 0.9730479587792311\n",
      "[I 2025-09-10 10:05:12,914] Trial 8 finished with value: 0.2 and parameters: {'eta': 0.038765388404987854, 'max_depth': 13, 'min_child_weight': 0.6801454495486001, 'gamma': 1.0376522903708274, 'lambda': 2.3561422767607767, 'alpha': 4.168927352371769, 'subsample': 0.7574008852079117, 'colsample_bytree': 0.6193143475086964, 'colsample_bylevel': 0.9583078730933395, 'max_bin': 64, 'sampling_method': 'gradient_based', 'aft_loss_distribution': 'extreme', 'aft_loss_distribution_scale': 2.3222768950185775, 'grow_policy': 'depthwise', 'booster': 'dart', 'rate_drop': 0.2987096000241063, 'skip_drop': 0.09385619757040344, 'normalize_type': 'forest', 'sample_type': 'weighted', 'n_estimators': 800}. Best is trial 2 with value: 0.20122047736449064.\n",
      "[Trial 008] state=COMPLETE value=0.2000 best=0.2012\n",
      "####################\n",
      "[10:09:00] Val AFT Total Expected Cost: 49566.0\n",
      "[10:09:00] Val AFT Balanced Accuracy: 0.2\n",
      "[10:09:00] Val AFT Accuracy: 0.005945303210463734\n",
      "[I 2025-09-10 10:09:00,806] Trial 9 finished with value: 0.2 and parameters: {'eta': 0.09864335887606307, 'max_depth': 7, 'min_child_weight': 30.433316201840512, 'gamma': 0.10817582247384327, 'lambda': 3.035856967579609, 'alpha': 0.0053505670340536155, 'subsample': 0.8187679528647974, 'colsample_bytree': 0.5402013225257882, 'colsample_bylevel': 0.8341661912786164, 'max_bin': 256, 'sampling_method': 'uniform', 'aft_loss_distribution': 'extreme', 'aft_loss_distribution_scale': 0.8243135739064495, 'grow_policy': 'lossguide', 'max_leaves': 36, 'booster': 'dart', 'rate_drop': 0.2052270362381225, 'skip_drop': 0.009948853606103092, 'normalize_type': 'tree', 'sample_type': 'uniform', 'n_estimators': 300}. Best is trial 2 with value: 0.20122047736449064.\n",
      "[Trial 009] state=COMPLETE value=0.2000 best=0.2012\n",
      "####################\n",
      "[10:09:03] Val AFT Total Expected Cost: 57400.0\n",
      "[10:09:03] Val AFT Balanced Accuracy: 0.2\n",
      "[10:09:03] Val AFT Accuracy: 0.9730479587792311\n",
      "[I 2025-09-10 10:09:03,854] Trial 10 finished with value: 0.2 and parameters: {'eta': 0.013823034986279485, 'max_depth': 4, 'min_child_weight': 4.62817136798396, 'gamma': 2.7526942539920465, 'lambda': 0.0004896762212829439, 'alpha': 0.00011136899422539573, 'subsample': 0.9866445737188785, 'colsample_bytree': 0.7019983322975066, 'colsample_bylevel': 0.7302133893574434, 'max_bin': 512, 'sampling_method': 'gradient_based', 'aft_loss_distribution': 'logistic', 'aft_loss_distribution_scale': 7.004681064723634, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'n_estimators': 50}. Best is trial 2 with value: 0.20122047736449064.\n",
      "[Trial 010] state=COMPLETE value=0.2000 best=0.2012\n",
      "####################\n",
      "[10:26:46] Val AFT Total Expected Cost: 49566.0\n",
      "[10:26:46] Val AFT Balanced Accuracy: 0.2\n",
      "[10:26:46] Val AFT Accuracy: 0.015061434799841459\n",
      "[I 2025-09-10 10:26:46,074] Trial 11 finished with value: 0.2 and parameters: {'eta': 0.021014493142790053, 'max_depth': 12, 'min_child_weight': 7.354865793639183, 'gamma': 2.22901757058419, 'lambda': 0.0037154066246453702, 'alpha': 25.537049912444182, 'subsample': 0.9700329631028007, 'colsample_bytree': 0.6584562069938976, 'colsample_bylevel': 0.7246668050071514, 'max_bin': 256, 'sampling_method': 'gradient_based', 'aft_loss_distribution': 'extreme', 'aft_loss_distribution_scale': 0.2146561805638634, 'grow_policy': 'lossguide', 'max_leaves': 472, 'booster': 'dart', 'rate_drop': 0.49262184200079256, 'skip_drop': 0.0058845747727971, 'normalize_type': 'tree', 'sample_type': 'uniform', 'n_estimators': 700}. Best is trial 2 with value: 0.20122047736449064.\n",
      "[Trial 011] state=COMPLETE value=0.2000 best=0.2012\n",
      "####################\n",
      "[10:27:10] Val AFT Total Expected Cost: 57406.0\n",
      "[10:27:10] Val AFT Balanced Accuracy: 0.203245078071962\n",
      "[10:27:10] Val AFT Accuracy: 0.9565992865636147\n",
      "[I 2025-09-10 10:27:10,784] Trial 12 finished with value: 0.203245078071962 and parameters: {'eta': 0.024911675839911256, 'max_depth': 14, 'min_child_weight': 10.104092851056944, 'gamma': 4.91005307357157, 'lambda': 0.007292943462377966, 'alpha': 10.715142145840296, 'subsample': 0.9259776597591033, 'colsample_bytree': 0.6118635623116906, 'colsample_bylevel': 0.6685377936752879, 'max_bin': 256, 'sampling_method': 'uniform', 'aft_loss_distribution': 'logistic', 'aft_loss_distribution_scale': 0.20481249382460925, 'grow_policy': 'lossguide', 'max_leaves': 169, 'booster': 'dart', 'rate_drop': 0.3981574558793888, 'skip_drop': 0.04875758282978792, 'normalize_type': 'tree', 'sample_type': 'uniform', 'n_estimators': 750}. Best is trial 12 with value: 0.203245078071962.\n",
      "[Trial 012] state=COMPLETE value=0.2032 best=0.2032\n",
      "####################\n",
      "[10:27:23] Val AFT Total Expected Cost: 57498.0\n",
      "[10:27:23] Val AFT Balanced Accuracy: 0.20593695287097585\n",
      "[10:27:23] Val AFT Accuracy: 0.8838684106222751\n",
      "[I 2025-09-10 10:27:24,002] Trial 13 finished with value: 0.20593695287097585 and parameters: {'eta': 0.022590605294964847, 'max_depth': 10, 'min_child_weight': 2.7742056232528873, 'gamma': 4.454889299937084, 'lambda': 0.00012895131838550495, 'alpha': 8.786772919249996, 'subsample': 0.89130764413329, 'colsample_bytree': 0.5112729288782607, 'colsample_bylevel': 0.7748667602260508, 'max_bin': 256, 'sampling_method': 'gradient_based', 'aft_loss_distribution': 'logistic', 'aft_loss_distribution_scale': 0.23941225376310016, 'grow_policy': 'lossguide', 'max_leaves': 146, 'booster': 'dart', 'rate_drop': 0.046455804743137985, 'skip_drop': 0.0494203492281816, 'normalize_type': 'tree', 'sample_type': 'uniform', 'n_estimators': 50}. Best is trial 13 with value: 0.20593695287097585.\n",
      "[Trial 013] state=COMPLETE value=0.2059 best=0.2059\n",
      "####################\n",
      "[10:29:09] Val AFT Total Expected Cost: 57296.0\n",
      "[10:29:09] Val AFT Balanced Accuracy: 0.1980040733197556\n",
      "[10:29:09] Val AFT Accuracy: 0.963337296868807\n",
      "[I 2025-09-10 10:29:09,313] Trial 14 finished with value: 0.1980040733197556 and parameters: {'eta': 0.02079371985083724, 'max_depth': 14, 'min_child_weight': 2.6293172150953326, 'gamma': 4.834767915202397, 'lambda': 0.00016209784325399638, 'alpha': 16.657917242630333, 'subsample': 0.9021223571063914, 'colsample_bytree': 0.5055670729188406, 'colsample_bylevel': 0.7784558847061787, 'max_bin': 256, 'sampling_method': 'uniform', 'aft_loss_distribution': 'logistic', 'aft_loss_distribution_scale': 0.20115624913435987, 'grow_policy': 'lossguide', 'max_leaves': 139, 'booster': 'dart', 'rate_drop': 0.006099819160103742, 'skip_drop': 0.04352767263899748, 'normalize_type': 'tree', 'sample_type': 'uniform', 'n_estimators': 650}. Best is trial 13 with value: 0.20593695287097585.\n",
      "[Trial 014] state=COMPLETE value=0.1980 best=0.2059\n",
      "####################\n",
      "[10:29:23] Val AFT Total Expected Cost: 59941.0\n",
      "[10:29:23] Val AFT Balanced Accuracy: 0.18675661914460284\n",
      "[10:29:23] Val AFT Accuracy: 0.8479984145858105\n",
      "[I 2025-09-10 10:29:23,063] Trial 15 finished with value: 0.18675661914460284 and parameters: {'eta': 0.010315777945602058, 'max_depth': 11, 'min_child_weight': 8.860544095832239, 'gamma': 4.98176328312936, 'lambda': 0.002600759888713199, 'alpha': 0.009906333070439973, 'subsample': 0.9195375856504526, 'colsample_bytree': 0.5738185149064864, 'colsample_bylevel': 0.6516962579447241, 'max_bin': 256, 'sampling_method': 'gradient_based', 'aft_loss_distribution': 'logistic', 'aft_loss_distribution_scale': 0.3905136998767427, 'grow_policy': 'lossguide', 'max_leaves': 120, 'booster': 'dart', 'rate_drop': 0.44555722125476943, 'skip_drop': 0.05827736891918151, 'normalize_type': 'tree', 'sample_type': 'weighted', 'n_estimators': 50}. Best is trial 13 with value: 0.20593695287097585.\n",
      "[Trial 015] state=COMPLETE value=0.1868 best=0.2059\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "\n",
    "study.optimize(lambda trial: aft_objective_acc(trial, y_val_class=labels_val, dtrain=dtrain, dval=dval),\n",
    "               n_trials=500,\n",
    "               callbacks=[progress_callback_totalcost],\n",
    "               show_progress_bar=True)\n",
    "\n",
    "# Logge das beste Ergebnis\n",
    "best_trial_acc = study.best_trial\n",
    "best_params_acc = best_trial_acc.params\n",
    "mlflow.log_params(best_params_acc)\n",
    "mlflow.log_metric(\"best_val_balanced_accuracy\", best_trial_acc.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beste Parameter nach realisierten Gesamtkosten:\", best_params_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d765b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(pd.DataFrame([best_params_acc]), ordner=\"05_model_input\", name=\"atf_best_params_balanced_accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb0def",
   "metadata": {},
   "source": [
    "### 3. Modellerstellung RSF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb6fd71",
   "metadata": {},
   "source": [
    "#### Daten vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053da92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rsf_model_input(df: pd.DataFrame, columns_to_drop: list, frag: float, class_column: str, sampling: bool) -> tuple[pd.DataFrame, np.ndarray]: \n",
    "    \"\"\" Prepares the input data for the Random Survival Forest model with option to sample a fraction of each class. \n",
    "    \n",
    "    Args: df (pd.DataFrame): The input dataframe containing features and target variables. \n",
    "    columns_to_drop (list): List of columns to drop from the dataframe. \n",
    "    frag (float): Fraction of data to sample from each class. \n",
    "    class_column (str): The name of the column representing the class labels. \n",
    "    sampling (bool): Whether to perform sampling or not.\n",
    "    Returns: tuple[pd.DataFrame, np.ndarray]: A tuple containing the feature dataframe and the structured array for survival analysis. \"\"\" \n",
    "    df_list = [] \n",
    "\n",
    "    if sampling:\n",
    "        for i in df[class_column].unique(): \n",
    "            df_list.append( df[df[class_column] == i].sample(frac=frag, random_state=42)) \n",
    "        df = pd.concat(df_list) \n",
    "\n",
    "    y_surv = Surv.from_arrays(event=df[\"event\"].astype(bool), time=df[\"duration\"].astype(float)) \n",
    "    X = df.drop(columns=columns_to_drop) \n",
    "    return X, y_surv \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31c0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train_surv = prepare_rsf_model_input(load_df(ordner=\"04_feature\", name = \"feature_train_corr_labels\").drop(columns=[\"upper_bound\"]), columns_to_drop=[\"duration\", \"event\", \"vehicle_id\", \"class\"], frag=1, class_column=\"class\", sampling=True) \n",
    "\n",
    "X_val, y_val_surv = prepare_rsf_model_input(load_df(ordner=\"04_feature\", name = \"feature_validation_corr_labels\").drop(columns=[\"upper_bound\"]), columns_to_drop=[\"duration\", \"event\", \"vehicle_id\", \"class_label\"], frag=1.0, class_column=\"class_label\", sampling=False) \n",
    "\n",
    "validation_data = load_df(ordner=\"04_feature\", name = \"feature_validation_corr_labels\").drop(columns=[\"upper_bound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sicherstellen, dass der Ordner existiert\n",
    "save_dir = \"../Data/05_model_input/RSF\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# X_train und X_val (DataFrames) speichern\n",
    "X_train.to_parquet(os.path.join(save_dir, \"X_train.parquet\"), index=False)\n",
    "X_val.to_parquet(os.path.join(save_dir, \"X_val.parquet\"), index=False)\n",
    "\n",
    "# y_train_surv und y_val_surv sind Structured Arrays -> DataFrame\n",
    "y_train_df = pd.DataFrame({\n",
    "    \"event\": y_train_surv[\"event\"].astype(int),\n",
    "    \"duration\": y_train_surv[\"time\"].astype(float)\n",
    "})\n",
    "y_val_df = pd.DataFrame({\n",
    "    \"event\": y_val_surv[\"event\"].astype(int),\n",
    "    \"duration\": y_val_surv[\"time\"].astype(float)\n",
    "})\n",
    "\n",
    "y_train_df.to_parquet(os.path.join(save_dir, \"y_train_surv.parquet\"), index=False)\n",
    "y_val_df.to_parquet(os.path.join(save_dir, \"y_val_surv.parquet\"), index=False)\n",
    "\n",
    "# Validation Data auch speichern\n",
    "validation_data.to_parquet(os.path.join(save_dir, \"validation_data.parquet\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11300f3d",
   "metadata": {},
   "source": [
    "#### Parameter festlegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae9acb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_rsf(df: pd.DataFrame) -> dict:  \n",
    "    \"\"\" \n",
    "    Berechnet die dynamischen Hyperparameter für RandomSurvivalForest basierend auf der Größe des Trainingsdatensatzes. \n",
    "\n",
    "    Args: \n",
    "        df (pd.DataFrame): Der Trainingsdatensatz.\n",
    "\n",
    "    Returns: dict: Ein Dictionary mit den berechneten Hyperparametern.\n",
    "    \"\"\"\n",
    "    N = len(X_train)\n",
    "\n",
    "    # Dynamische Ableitung aus deinem Grid\n",
    "    min_samples_leaf = int(N * 0.01)\n",
    "    min_samples_split = 2 * min_samples_leaf\n",
    "    max_depth = 16\n",
    "\n",
    "    best_params = {\n",
    "        \"n_estimators\": 8,\n",
    "        \"max_depth\": max_depth,\n",
    "        \"max_features\": \"sqrt\",\n",
    "        \"min_samples_leaf\": min_samples_leaf,\n",
    "        \"min_samples_split\": min_samples_split\n",
    "    }\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b84ae997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_rsf_and_log(\n",
    "    X_train: pd.DataFrame, y_train_surv: Surv,\n",
    "    experiment_name: str = \"RSF_final\",\n",
    "    model_name: str = \"Rsf_final_model\",\n",
    "    local_model_path: str = \"../data/06_models/RSF_final_model_test\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and log the final Random Survival Forest model.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): The training features.\n",
    "        y_train_surv (Surv): The training survival data.\n",
    "        experiment_name (str): The name of the MLflow experiment.\n",
    "        model_name (str): The name of the model to register in MLflow.\n",
    "        local_model_path (str): Local path to save the trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    best_params = param_rsf(X_train)\n",
    "\n",
    "    with mlflow.start_run(run_name=experiment_name):\n",
    "        mlflow.log_params(best_params)\n",
    "\n",
    "        rsf = RandomSurvivalForest(\n",
    "            n_estimators=best_params[\"n_estimators\"],\n",
    "            max_depth=best_params[\"max_depth\"],\n",
    "            max_features=best_params[\"max_features\"],\n",
    "            min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "            min_samples_split=best_params[\"min_samples_split\"],\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        rsf.fit(X_train, y_train_surv)\n",
    "\n",
    "        # Logge Modell bei MLflow\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=rsf,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=model_name\n",
    "        )\n",
    "\n",
    "        # Speicher lokal mit joblib\n",
    "        os.makedirs(local_model_path, exist_ok=True)\n",
    "        joblib.dump(rsf, os.path.join(local_model_path, \"rsf_model.joblib\"))\n",
    "\n",
    "        print(\"RSF Training abgeschlossen, in MLflow & lokal gespeichert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9095e33",
   "metadata": {},
   "source": [
    "#### Anwendung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e2bd54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 15:36:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/09/09 15:36:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'Rsf_final_model_test' already exists. Creating a new version of this model...\n",
      "Created version '3' of model 'Rsf_final_model_test'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RSF Training abgeschlossen, in MLflow & lokal gespeichert.\n"
     ]
    }
   ],
   "source": [
    "train_final_rsf_and_log(X_train, y_train_surv, experiment_name=\"RSF_final_test\", model_name=\"Rsf_final_model_test\", local_model_path=\"../data/06_models/RSF_final_model_test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f88614",
   "metadata": {},
   "source": [
    "### 4. Modellerstellung XGBosst mit AFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e358a",
   "metadata": {},
   "source": [
    "#### Daten vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "397cb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdtrain = prepare_rsf_model_input(load_df(ordner=\"04_feature\", name = \"feature_train_corr_labels\"), columns_to_drop=[\"duration\", \"event\", \"vehicle_id\", \"class\", \"upper_bound\"])\n",
    "xdval   = prepare_rsf_model_input(load_df(ordner=\"04_feature\", name = \"feature_validation_corr_labels\"), columns_to_drop=[\"duration\", \"event\", \"vehicle_id\", \"class_label\", \"upper_bound\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b711883",
   "metadata": {},
   "source": [
    "#### Parameter vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90192f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_params_df_aft() -> tuple[dict, int]:\n",
    "    \"\"\"\n",
    "    Liest die besten Parameter aus den HPOs und gibt sie als DataFrame zurück.\n",
    "\n",
    "    return: pd.DataFrame mit den besten Parametern.\n",
    "    \"\"\"\n",
    "\n",
    "    best_params = load_df(ordner=\"05_model_input\", name=\"atf_best_params_totalcost\").iloc[0].to_dict()\n",
    "\n",
    "    params = {\n",
    "        \"tree_method\": \"hist\",             # oder \"gpu_hist\" je nach Version\n",
    "        \"device\": \"cuda\",\n",
    "        \"objective\": \"survival:aft\",\n",
    "        \"aft_loss_distribution\": best_params[\"aft_loss_distribution\"],     # 'logistic'\n",
    "        \"aft_loss_distribution_scale\": float(best_params[\"aft_loss_distribution_scale\"]),\n",
    "        \"eta\": float(best_params[\"learning_rate\"]),\n",
    "        \"max_depth\": int(best_params[\"max_depth\"]),\n",
    "        \"min_child_weight\": int(best_params[\"min_child_weight\"]),\n",
    "        \"reg_lambda\": float(best_params[\"lambda\"]),\n",
    "        \"reg_alpha\": float(best_params[\"alpha\"]),\n",
    "        \"subsample\": float(best_params[\"subsample\"]),\n",
    "        \"colsample_bytree\": float(best_params[\"colsample_bytree\"]),\n",
    "        # booster kannst du weglassen oder explizit auf \"gbtree\" setzen\n",
    "        \"verbosity\": 1\n",
    "    }\n",
    "\n",
    "    num_boost_round = int(best_params[\"n_estimators\"])\n",
    "\n",
    "    return params, num_boost_round\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb83e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fertig. Modell lokal gespeichert unter: data/06_models/XGB_AFT_final_model\n"
     ]
    }
   ],
   "source": [
    "def train_final_aft_and_log(\n",
    "    xdtrain: xgb.DMatrix, xdval: xgb.DMatrix, experiment_name: str = \"XGB_AFT_final\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the final XGBoost AFT model and log the results.\n",
    "\n",
    "    Args:\n",
    "        xdtrain (xgb.DMatrix): The training data for the final model.\n",
    "        xdval (xgb.DMatrix): The validation data for early stopping.\n",
    "        best_params (dict): The best hyperparameters from the Optuna study.\n",
    "        num_boost_round (int): The number of boosting rounds.\n",
    "        experiment_name (str): The name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "        xgb.Booster: The trained XGBoost Booster model.\n",
    "    \"\"\"\n",
    "    params, num_boost_round = prepare_params_df_aft()\n",
    "    setup_mlflow(experiment_name)\n",
    "    early_stopping_rounds = 50  # Du kannst das anpassen oder entfernen, wenn nicht gewünscht\n",
    "    with mlflow.start_run(run_name=\"XGB_AFT_final\"):\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=xdtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            evals=[(xdval, \"val\")],              # für Early Stopping; entferne diese Zeile + early_stopping_rounds, falls unerwünscht\n",
    "            early_stopping_rounds=early_stopping_rounds,\n",
    "            verbose_eval=50\n",
    "        )\n",
    "\n",
    "    # In MLflow loggen (Artefakt)\n",
    "    mlflow.xgboost.log_model(\n",
    "        xgb_model=booster,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=\"AFT_final_model\"  # optional: Modellregistry\n",
    "    )\n",
    "\n",
    "    # Lokal im MLflow-Format speichern\n",
    "    local_path = \"../data/06_models/AFT_final_model\"\n",
    "    mlflow.xgboost.save_model(xgb_model=booster, path=local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd077c",
   "metadata": {},
   "source": [
    "#### Anwendung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc1d23c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 07:48:25 INFO mlflow.tracking.fluent: Experiment with name 'XGB_AFT_final' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: file:///workspace/mlruns\n",
      "[0]\tval-aft-nloglik:0.67713\n",
      "[50]\tval-aft-nloglik:16.08798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/08 07:48:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:158: UserWarning: [07:48:27] WARNING: /home/coder/xgboost/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\u001b[31m2025/09/08 07:48:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'AFT_final_model' already exists. Creating a new version of this model...\n",
      "Created version '3' of model 'AFT_final_model'.\n",
      "/usr/local/lib/python3.12/dist-packages/xgboost/core.py:158: UserWarning: [07:48:31] WARNING: /home/coder/xgboost/src/c_api/c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_final_aft_and_log(xdtrain, xdval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
