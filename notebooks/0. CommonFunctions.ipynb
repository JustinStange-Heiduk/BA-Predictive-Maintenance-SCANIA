{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c9814d",
   "metadata": {},
   "source": [
    "# Predictive Maintenance mit SCANIA-Daten – Common Functions to load\n",
    "\n",
    "**Projekt:** Bachelorarbeit Data Science  \n",
    "**Thema:** \n",
    "**Datengrundlage:** SCANIA Component X Dataset  \n",
    "**Autor:** Justin Stange-Heiduk  \n",
    "**Betreuung:** Dr. Martin Prause  \n",
    "**Ziel:** Erstellen und testen der Daten Vorbereitung Funktionen  \n",
    "\n",
    "---\n",
    "\n",
    "**Erstellt:** 2025-08-19   \n",
    "**Letzte Änderung:** 2025-07-25\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01563f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from sksurv.util import Surv\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7387be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df: pd.DataFrame, ordner: str, name: str) -> None:\n",
    "    \"\"\"\n",
    "    Speichert ein DataFrame als Parquet-Datei im angegebenen Ordner.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Das zu speichernde DataFrame.\n",
    "        ordner (str): Der Ordner, in dem die Parquet-Datei gespeichert werden soll.\n",
    "        name (str): Der Name der Parquet-Datei (ohne .parquet)\n",
    "    \"\"\"\n",
    "\n",
    "    df.to_parquet(f\"../data/{ordner}/{name}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20db674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(ordner: str, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lädt ein DataFrame aus einer Parquet-Datei im angegebenen Ordner.\n",
    "\n",
    "    Args:\n",
    "        ordner (str): Der Ordner, in dem die Parquet-Datei gespeichert ist. (../data/{ordner})\n",
    "        name (str): Der Name der Parquet-Datei (ohne .parquet)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Das geladene DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.read_parquet(f\"../data/{ordner}/{name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5334e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_dd(ordner: str, name: str) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lädt ein DataFrame aus einer Parquet-Datei im angegebenen Ordner.\n",
    "\n",
    "    Args:\n",
    "        ordner (str): Der Ordner, in dem die Parquet-Datei gespeichert ist. (../data/{ordner})\n",
    "        name (str): Der Name der Parquet-Datei (ohne .parquet)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Das geladene DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    return dd.read_parquet(f\"../data/{ordner}/{name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd868f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_raw_data() -> dict:\n",
    "    \"\"\"\n",
    "    Load raw data from a CSV file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The loaded raw data.\n",
    "    \"\"\"\n",
    "    test_labels = pd.read_csv(\"../data/01_raw/test_labels.csv\")\n",
    "    test_operational = pd.read_csv(\"../data/01_raw/test_operational_readouts.csv\")\n",
    "    test_specifications = pd.read_csv(\"../data/01_raw/test_specifications.csv\")\n",
    "\n",
    "    train_tte = pd.read_csv(\"../data/01_raw/train_tte.csv\")\n",
    "    train_operational = pd.read_csv(\"../data/01_raw/train_operational_readouts.csv\")\n",
    "    train_specifications = pd.read_csv(\"../data/01_raw/train_specifications.csv\")\n",
    "\n",
    "    validation_tte = pd.read_csv(\"../data/01_raw/validation_labels.csv\")\n",
    "    validation_operational = pd.read_csv(\"../data/01_raw/validation_operational_readouts.csv\")\n",
    "    validation_specifications = pd.read_csv(\"../data/01_raw/validation_specifications.csv\")\n",
    "\n",
    "    return dict({\"test\": {\"labels\": test_labels, \"readouts\": test_operational, \"spec\": test_specifications},\n",
    "           \"train\": {\"tte\": train_tte, \"readouts\": train_operational, \"spec\": train_specifications},\n",
    "           \"validation\": {\"labels\": validation_tte, \"readouts\": validation_operational, \"spec\": validation_specifications}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8d1f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_specific_raw_data(name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load specific raw data from CSV files based on the data type.\n",
    "\n",
    "    Args:\n",
    "        name (str): The type of data to load. Options are:\n",
    "        test_labels, test_operational_readouts, test_specifications, \n",
    "        train_tte, train_operational_readouts, train_specifications, \n",
    "        validation_labels, validation_operational_readouts, validation_specifications.\n",
    "\n",
    "    Returns:\n",
    "        dict: The loaded raw data for the specified data type.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(f\"../data/01_raw/{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "977719f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_rsf_model_input(df: pd.DataFrame, columns_to_drop: list, frag: float, class_column: str, sampling: bool) -> tuple[pd.DataFrame, np.ndarray]: \n",
    "    \"\"\" Prepares the input data for the Random Survival Forest model with option to sample a fraction of each class. \n",
    "    \n",
    "    Args: df (pd.DataFrame): The input dataframe containing features and target variables. \n",
    "    columns_to_drop (list): List of columns to drop from the dataframe. \n",
    "    frag (float): Fraction of data to sample from each class. \n",
    "    class_column (str): The name of the column representing the class labels. \n",
    "    sampling (bool): Whether to perform sampling or not.\n",
    "    Returns: tuple[pd.DataFrame, np.ndarray]: A tuple containing the feature dataframe and the structured array for survival analysis. \"\"\" \n",
    "    df_list = [] \n",
    "\n",
    "    if sampling:\n",
    "        for i in df[class_column].unique(): \n",
    "            df_list.append( df[df[class_column] == i].sample(frac=frag, random_state=42)) \n",
    "        df = pd.concat(df_list) \n",
    "\n",
    "    y_surv = Surv.from_arrays(event=df[\"event\"].astype(bool), time=df[\"duration\"].astype(float)) \n",
    "    X = df.drop(columns=columns_to_drop) \n",
    "    return X, y_surv \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_aft_model_input(df: pd.DataFrame, columns_to_drop) -> pd.DataFrame: \n",
    "    \"\"\" Prepares the input data for the XGBoost model with aft. \n",
    "    \n",
    "    Args: \n",
    "    df (pd.DataFrame): The input dataframe containing features and target variables. \n",
    "    columns_to_drop (list): List of columns to drop from the dataframe. \n",
    "\n",
    "    Return:\n",
    "    pd.DataFrame: The feature dataframe for XGBoost. \n",
    "    \"\"\"\n",
    "\n",
    "    y = {\n",
    "        \"lower_bound\": df[\"duration\"].astype(float),\n",
    "        \"upper_bound\":  df[\"upper_bound\"].astype(float),\n",
    "    }\n",
    "\n",
    "    x = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    d = xgb.DMatrix(data=x, label_lower_bound=y[\"lower_bound\"],\n",
    "                     label_upper_bound=y[\"upper_bound\"])\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "631db2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_and_taus()-> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\" Returns the cost matrix and class boundaries (taus) for RUL classification.    \n",
    "    # Kostenmatrix aus deinem Paper (Zeilen = Actual n, Spalten = Predicted m)\n",
    "    Returns: tuple[np.ndarray, np.ndarray]: A tuple containing the cost matrix and class boundaries. \n",
    "    \"\"\"\n",
    "    COST = np.array([\n",
    "        [0,   7,   8,   9,   10],\n",
    "        [200, 0,   7,   8,    9],\n",
    "        [300, 200, 0,   7,    8],\n",
    "        [400, 300, 200, 0,    7],\n",
    "        [500, 400, 300, 200,  0]\n",
    "    ], dtype=float)\n",
    "\n",
    "    # Klassengrenzen für RUL in Zeiteinheiten, konsistent zu deinen Labels 0..4\n",
    "    # Beispiel: 4: [0,6), 3: [6,12), 2: [12,24), 1: [24,48), 0: [48, inf)\n",
    "    TAUS = np.array([6.0, 12.0, 24.0, 48.0], dtype=float)\n",
    "\n",
    "    return COST, TAUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a912f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path: str) -> None:\n",
    "    \"\"\"Erzeugt das Zielverzeichnis, falls es nicht existiert.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
