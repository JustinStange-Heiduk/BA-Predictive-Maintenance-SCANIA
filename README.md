# ðŸš› SCANIA Component X â€“ Predictive Maintenance Projekt

## 1. Ãœberblick

Dieses Projekt implementiert einen vollstÃ¤ndigen **Predictive Maintenance Workflow** fÃ¼r das **SCANIA Component X Dataset**.  
Ziel ist die **Vorhersage der Restlebensdauer (Remaining Useful Life, RUL)** von Fahrzeugkomponenten mittels **Survival-Analyse** und **klassischer Machine-Learning-Modelle**.

Besonderheiten:
- Zeitreihenbasiertes Feature Engineering mit **Sliding Windows** und **tsfresh**
- Survival-Modelle: **Random Survival Forest (RSF)**, **XGBoost AFT**
- Modellbewertung mit **Kostenmatrix** (realistische Kosten fÃ¼r FehleinschÃ¤tzungen)
- Experiment-Tracking via **MLflow**
- Interaktive **Streamlit-App** fÃ¼r Deployment
- Explainability mit **Permutation Importance** und **GPT-basierter natÃ¼rlicher SpracherklÃ¤rung**

---

## 2. Datenquelle

Die Daten stammen aus dem Ã¶ffentlich zugÃ¤nglichen Forschungskatalog des  
**Swedish National Data Service (SND)**:

> **Titel:** SCANIA Component X Dataset  
> **Quelle:** [researchdata.se â€“ Dataset 2024-34/2](https://researchdata.se/en/catalogue/dataset/2024-34/2)  
> **Zugriffsdatum:** 14. Juli 2025  
> **Lizenz:** CC BY 4.0 (Creative Commons Attribution)

Das Dataset umfasst neun CSV-Dateien fÃ¼r Training, Validierung und Test:

data/01_raw/
â”œâ”€â”€ train_specifications.csv
â”œâ”€â”€ train_operational_readouts.csv
â”œâ”€â”€ train_tte.csv
â”œâ”€â”€ validation_specifications.csv
â”œâ”€â”€ validation_operational_readouts.csv
â”œâ”€â”€ validation_labels.csv
â”œâ”€â”€ test_specifications.csv
â”œâ”€â”€ test_operational_readouts.csv
â”œâ”€â”€ test_labels.csv

yaml
Code kopieren

---

## 3. Daten-Pipeline

Die Daten werden in mehreren Stufen verarbeitet:

| Ordner             | Zweck                                                                 | Beispiele                           |
|--------------------|----------------------------------------------------------------------|------------------------------------|
| **01_raw**         | UnverÃ¤nderte Rohdaten                                                | Original CSV-Dateien von SND        |
| **02_intermediate**| Vorverarbeitete Daten (Interpolation, Differenzenbildung, `dropna`)  | interpolated_readouts.parquet       |
| **03_primary**     | Integrierte, bereinigte Struktur (Window-basiert)                    | windows_8.csv, windows_32.csv       |
| **04_feature**     | Von **tsfresh** generierte Features                                  | features_window8.csv, features_window32.csv |
| **05_model_input** | Finalisierte Inputs fÃ¼r Modelle nach Feature Selection               | X_train.parquet, X_val.parquet      |
| **06_models**      | Gespeicherte Modelle                                                 | rsf_model.joblib, aft_model.json    |
| **07_model_output**| Modelloutputs (Predictions, Residuen, Wahrscheinlichkeiten)          | y_pred.csv, rul_pred.csv, shap.csv  |
| **08_reporting**   | Reports, Plots, Explainability                                       | ROC-Curve.png, feature_importance.csv |

---

## 4. Modellierung

### Feature Engineering
- Sliding Windows (GrÃ¶ÃŸe: 4, 8, 16, 32, 64)
- **tsfresh** Feature-Extraktion (MinimalFCParameters)
- Feature Selection via Kendallâ€™s Ï„ und Pearson-Korrelation

### Modelle
- **Random Survival Forest (RSF)** â€“ fÃ¼r Survival-Analyse und Klassifikation
- **XGBoost AFT (Accelerated Failure Time)** â€“ fÃ¼r RUL-Intervallvorhersage

### Evaluierung
- Kostenmatrix-basierte Entscheidung  
- Klassengrenzen (TAUs) â†’ RUL-Klassen 0â€“4  
- Metriken: Accuracy, Balanced Accuracy, Expected Cost, Confusion Matrix

---

## 5. Tools & Libraries

- **Python 3.12**
- **tsfresh** â€“ Feature-Engineering
- **scikit-survival**, **skranger** â€“ Survival-Modelle
- **XGBoost** â€“ AFT Modell
- **MLflow** â€“ Experiment-Tracking
- **Streamlit** â€“ Deployment UI
- **SHAP, Permutation Importance** â€“ Explainability
- **OpenAI GPT-4o-mini** â€“ NatÃ¼rliche Sprach-ErklÃ¤rungen der Modellentscheidungen

---

## 6. Experiment-Tracking mit MLflow

Alle Trainings- und EvaluierungslÃ¤ufe werden mit **MLflow** protokolliert.  
Start der UI:

```bash
mlflow ui --backend-store-uri ./mlruns --host 0.0.0.0 --port 5000
7. Deployment
Das Projekt enthÃ¤lt eine interaktive Streamlit-App (src/app.py):

bash
Code kopieren
streamlit run /workspace/src/app.py
Features:

CSV-Upload eines einzelnen Readouts

Preprocessing-Pipeline (Interpolation, Differenzenbildung, Sliding Windows, tsfresh)

Modellvorhersage (RSF)

Visualisierung der Klassenscores (pâ‚€â€“pâ‚„)

Entscheidung basierend auf Kostenmatrix oder Argmax

GPT-gestÃ¼tzte ErklÃ¤rungen der Modellentscheidung

8. Projektstruktur
bash
Code kopieren
.
â”œâ”€â”€ data/                 # Datenpipeline (01_raw â€“ 08_reporting)
â”œâ”€â”€ notebooks/            # Jupyter Notebooks (EDA, Modeling, Evaluation, Deployment)
â”œâ”€â”€ src/                  # Python-Module (Preprocessing, Modelle, Streamlit-App)
â”‚   â”œâ”€â”€ app.py            # Streamlit-App
â”‚   â”œâ”€â”€ decision_utils.py # Entscheidungslogik (Kostenmatrix, Klassenwahrscheinlichkeit)
â”‚   â”œâ”€â”€ preprocessing.py  # Datenaufbereitung
â”‚   â”œâ”€â”€ local_feature_utils.py # Explainability (z.B. Feature Importance)
â”‚   â”œâ”€â”€ openai_utils.py   # GPT-basierte ErklÃ¤rungen
â”œâ”€â”€ mlruns/               # MLflow Tracking Daten
â”œâ”€â”€ env/.env              # API Keys (nicht versioniert)
â”œâ”€â”€ Dockerfile            # Container-Build
â”œâ”€â”€ docker-compose.yml    # Orchestrierung
â”œâ”€â”€ requirements.txt      # Python Dependencies
â””â”€â”€ README.md             # Projektdokumentation
9. Reproduzierbarkeit & Container
Das gesamte Projekt kann mit Docker ausgefÃ¼hrt werden:

bash
Code kopieren
docker-compose up --build
10. Lizenz
Daten: CC BY 4.0 (SCANIA Component X Dataset, SND)

Code: MIT License (siehe Repository)